{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports r√©ussis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_trading_env  # IMPORTANT: Pour enregistrer l'environnement\n",
    "\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "import wandb\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration WandB et des Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 1 configurations √† tester\n",
      "   WandB: Activ√©\n"
     ]
    }
   ],
   "source": [
    "# WandB (mettre False pour d√©sactiver)\n",
    "USE_WANDB = True  \n",
    "\n",
    "# Liste des configurations √† tester\n",
    "TEST_CONFIGS = [\n",
    "    # # Test 1 : Simple Return (baseline)\n",
    "    # {\n",
    "    #     \"name\": \"simple_return_baseline\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"simple_return\",\n",
    "    #     \"risk_penalty\": 0.0,\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,  # ‚Üê R√©duire √† 5000 pour test ultra-rapide\n",
    "    # },\n",
    "    \n",
    "    # # Test 2 : Sharpe Conservateur\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_conservative\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.3,\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 1e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 3 : Sharpe √âquilibr√©\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_balanced\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.1,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 4 : Sharpe Agressif\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_aggressive\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.05,\n",
    "    #     \"reward_scaling\": 2.0,\n",
    "    #     \"learning_rate\": 5e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 5 : Momentum-Based\n",
    "    # {\n",
    "    #     \"name\": \"momentum_aggressive\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"momentum_based\",\n",
    "    #     \"risk_penalty\": 0.05,\n",
    "    #     \"reward_scaling\": 2.0,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 6 : Profit avec P√©nalit√© Drawdown\n",
    "    # {\n",
    "    #     \"name\": \"profit_with_drawdown\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"profit_drawdown\",\n",
    "    #     \"risk_penalty\": 0.5,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # Test 7 : SAC avec Simple Return\n",
    "    # {\n",
    "    #     \"name\": \"sac_simple\",\n",
    "    #     \"algo\": \"SAC\",\n",
    "    #     \"reward_type\": \"simple_return\",\n",
    "    #     \"risk_penalty\": 0.0,\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 8 : SAC avec Sharpe\n",
    "    # {\n",
    "    #     \"name\": \"sac_sharpe\",\n",
    "    #     \"algo\": \"SAC\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.1,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "     {\n",
    "        \"name\": \"sharpe_balanced_v2\",\n",
    "        \"algo\": \"PPO\",\n",
    "        \"reward_type\": \"clipped_sharpe\",\n",
    "        \"risk_penalty\": 0.1,\n",
    "        \"reward_scaling\": 1.5,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 10,\n",
    "        \"ent_coef\": 0.01,  # Exploration\n",
    "        \"timesteps\": 500000,  # 50k pour meilleurs r√©sultats\n",
    "    },\n",
    "    \n",
    "    # # Test 2 : Sharpe Plus Agressif (moins de p√©nalit√© risque)\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_aggressive_v2\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.05,  # ‚Üê Plus faible = plus agressif\n",
    "    #     \"reward_scaling\": 2.0,  # ‚Üê Plus de scaling\n",
    "    #     \"learning_rate\": 5e-4,  # ‚Üê Learning rate plus √©lev√©\n",
    "    #     \"n_steps\": 2048,\n",
    "    #     \"batch_size\": 64,\n",
    "    #     \"n_epochs\": 10,\n",
    "    #     \"ent_coef\": 0.02,  # Plus d'exploration\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 3 : Sharpe Conservateur (plus de p√©nalit√© risque)\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_conservative_v2\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.2,  # ‚Üê Plus √©lev√© = plus conservateur\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 1e-4,  # ‚Üê Learning rate plus faible\n",
    "    #     \"n_steps\": 4096,  # ‚Üê Plus de steps = plus stable\n",
    "    #     \"batch_size\": 128,  # ‚Üê Batch plus grand\n",
    "    #     \"n_epochs\": 15,  # ‚Üê Plus d'√©poques\n",
    "    #     \"ent_coef\": 0.005,  # Moins d'exploration\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "   \n",
    "    \n",
    "    # # Sortino Ratio (p√©nalise seulement downside)\n",
    "    # {\n",
    "    #     \"name\": \"sortino_balanced\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"sortino_ratio\",\n",
    "    #     \"risk_penalty\": 0.1,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"n_steps\": 2048,\n",
    "    #     \"batch_size\": 64,\n",
    "    #     \"n_epochs\": 10,\n",
    "    #     \"ent_coef\": 0.01,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "   \n",
    "    \n",
    "    # # Simple Return avec Hyperparam√®tres Optimis√©s\n",
    "    # {\n",
    "    #     \"name\": \"simple_return_optimized\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"simple_return\",\n",
    "    #     \"risk_penalty\": 0.0,  # Pas utilis√©\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"n_steps\": 2048,\n",
    "    #     \"batch_size\": 64,\n",
    "    #     \"n_epochs\": 10,\n",
    "    #     \"ent_coef\": 0.015,  # Un peu plus d'exploration\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "   \n",
    "    # # Sharpe + Momentum Hybride\n",
    "    # {\n",
    "    #     \"name\": \"hybrid_sharpe_momentum\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"hybrid_sharpe_momentum\", \n",
    "    #     \"risk_penalty\": 0.1,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"n_steps\": 2048,\n",
    "    #     \"batch_size\": 64,\n",
    "    #     \"n_epochs\": 10,\n",
    "    #     \"ent_coef\": 0.01,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ {len(TEST_CONFIGS)} configurations √† tester\")\n",
    "print(f\"   WandB: {'Activ√©' if USE_WANDB else 'D√©sactiv√©'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de Pr√©traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trouv√© 9 datasets\n"
     ]
    }
   ],
   "source": [
    "def load_pkl(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_pickle(path)\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        for col in [\"datetime\", \"date\", \"time\", \"timestamp\", \"Timestamp\"]:\n",
    "            if col in df.columns:\n",
    "                df = df.set_index(pd.to_datetime(df[col]))\n",
    "                break\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_log_return(df: pd.DataFrame, col: str = \"close\") -> pd.Series:\n",
    "    return np.log(df[col]).diff()\n",
    "\n",
    "\n",
    "def feature_moving_average(df: pd.DataFrame, col: str = \"close\", window: int = 20) -> pd.Series:\n",
    "    return df[col].rolling(window).mean()\n",
    "\n",
    "\n",
    "def feature_volatility(df: pd.DataFrame, col: str = \"close\", window: int = 20) -> pd.Series:\n",
    "    return df[col].pct_change().rolling(window).std()\n",
    "\n",
    "\n",
    "def feature_RSI(df: pd.DataFrame, col: str = \"close\", window: int = 14) -> pd.Series:\n",
    "    delta = df[col].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    ma_up = up.rolling(window=window).mean()\n",
    "    ma_down = down.rolling(window=window).mean()\n",
    "    rs = ma_up / (ma_down + 1e-9)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def feature_MACD(df: pd.DataFrame, col: str = \"close\", fast: int = 12, slow: int = 26, signal: int = 9):\n",
    "    ema_fast = df[col].ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = df[col].ewm(span=slow, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    macd_signal = macd.ewm(span=signal, adjust=False).mean()\n",
    "    return macd, macd_signal\n",
    "\n",
    "\n",
    "def feature_OBV(df: pd.DataFrame, col_close: str = \"close\", col_volume: str = \"volume\") -> pd.Series:\n",
    "    if col_volume not in df.columns:\n",
    "        return pd.Series(0, index=df.index)\n",
    "    obv = [0]\n",
    "    for i in range(1, len(df)):\n",
    "        if df[col_close].iat[i] > df[col_close].iat[i - 1]:\n",
    "            obv.append(obv[-1] + df[col_volume].iat[i])\n",
    "        elif df[col_close].iat[i] < df[col_close].iat[i - 1]:\n",
    "            obv.append(obv[-1] - df[col_volume].iat[i])\n",
    "        else:\n",
    "            obv.append(obv[-1])\n",
    "    return pd.Series(obv, index=df.index)\n",
    "\n",
    "\n",
    "def zscore(series: pd.Series) -> pd.Series:\n",
    "    return (series - series.mean()) / (series.std() + 1e-9)\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.sort_index().drop_duplicates().dropna(how=\"all\")\n",
    "    \n",
    "    if \"close\" not in df.columns:\n",
    "        for c in [\"Close\", \"Adj Close\", \"adjclose\", \"adj_close\"]:\n",
    "            if c in df.columns:\n",
    "                df[\"close\"] = df[c]\n",
    "                break\n",
    "    if \"volume\" not in df.columns:\n",
    "        df[\"volume\"] = 0\n",
    "\n",
    "    # Features\n",
    "    df[\"feature_log_return\"] = feature_log_return(df, \"close\")\n",
    "    df[\"feature_RSI\"] = feature_RSI(df, \"close\")\n",
    "    macd, macd_signal = feature_MACD(df, \"close\")\n",
    "    df[\"feature_MACD\"] = macd\n",
    "    df[\"feature_MACD_signal\"] = macd_signal\n",
    "    \n",
    "    for w in [5, 20, 50]:\n",
    "        df[f\"feature_ma_{w}\"] = feature_moving_average(df, \"close\", w)\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"feature_vol_{w}\"] = feature_volatility(df, \"close\", w)\n",
    "    df[\"feature_OBV\"] = feature_OBV(df, \"close\", \"volume\")\n",
    "\n",
    "    # Normalisation\n",
    "    feat_cols = [c for c in df.columns if c.startswith(\"feature_\")]\n",
    "    for c in feat_cols:\n",
    "        df[c] = df[c].ffill().fillna(0) \n",
    "        df[c] = zscore(df[c])\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "# V√©rifier les donn√©es\n",
    "dataset_dir = \"data/*.pkl\"\n",
    "files = sorted(glob.glob(dataset_dir))\n",
    "print(f\"‚úÖ Trouv√© {len(files)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de R√©compense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 7 fonctions de r√©compense disponibles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def reward_simple_return(history, **kwargs) -> float:\n",
    "    \"\"\"REWARD 1 : Simple rendement en pourcentage\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    pct_change = (current_value - previous_value) / previous_value\n",
    "    return np.clip(pct_change * 100, -10, 10)\n",
    "\n",
    "\n",
    "def reward_clipped_sharpe(history, risk_penalty=0.1, reward_scaling=1.0) -> float:\n",
    "    \"\"\"REWARD 2 : Ratio de Sharpe clipp√©\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0 or current_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    instant_log_return = np.log(current_value / previous_value)\n",
    "    instant_log_return = np.clip(instant_log_return, -0.1, 0.1)\n",
    "    \n",
    "    WINDOW = 20\n",
    "    all_values_np = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    safe_values = np.where(all_values_np <= 0, 1e-9, all_values_np)\n",
    "    \n",
    "    if len(safe_values) > 1:\n",
    "        log_returns = np.diff(np.log(safe_values))\n",
    "    else:\n",
    "        log_returns = np.array([0.0])\n",
    "    \n",
    "    if len(log_returns) >= WINDOW:\n",
    "        volatility = np.std(log_returns[-WINDOW:])\n",
    "    else:\n",
    "        volatility = np.std(log_returns) if len(log_returns) > 1 else 1e-9\n",
    "    \n",
    "    volatility = max(volatility, 0.001)\n",
    "    sharpe = instant_log_return / (volatility * risk_penalty)\n",
    "    sharpe = np.clip(sharpe, -10, 10)\n",
    "    \n",
    "    return sharpe * reward_scaling\n",
    "\n",
    "\n",
    "def reward_momentum_based(history, risk_penalty=0.05, reward_scaling=2.0) -> float:\n",
    "    \"\"\"REWARD 3 : Bas√© sur le momentum\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    values = np.asarray(history['portfolio_valuation'][-10:], dtype=np.float64)\n",
    "    safe_values = np.where(values <= 0, 1e-9, values)\n",
    "    \n",
    "    if len(safe_values) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    returns = np.diff(np.log(safe_values))\n",
    "    momentum = np.mean(returns[-5:]) if len(returns) >= 5 else np.mean(returns)\n",
    "    momentum_bonus = np.tanh(momentum * 10) * reward_scaling\n",
    "    \n",
    "    instant_return = np.log(safe_values[-1] / safe_values[-2])\n",
    "    instant_return = np.clip(instant_return, -0.1, 0.1)\n",
    "    \n",
    "    total_reward = (instant_return * 100) + momentum_bonus\n",
    "    return np.clip(total_reward, -10, 10)\n",
    "\n",
    "\n",
    "def reward_profit_drawdown(history, risk_penalty=0.5, reward_scaling=1.5) -> float:\n",
    "    \"\"\"REWARD 4 : Profit avec p√©nalit√© pour drawdown\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    pct_change = (current_value - previous_value) / previous_value\n",
    "    \n",
    "    all_values = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    all_values = np.where(all_values <= 0, 1e-9, all_values)\n",
    "    \n",
    "    peak_value = np.max(all_values)\n",
    "    drawdown = (current_value - peak_value) / peak_value\n",
    "    \n",
    "    reward = (pct_change * 100) - (abs(drawdown) * 100 * risk_penalty)\n",
    "    return np.clip(reward, -10, 10) * reward_scaling\n",
    "\n",
    "\n",
    "def reward_sortino_ratio(history, risk_penalty=0.1, reward_scaling=1.0) -> float:\n",
    "    \"\"\"REWARD 5 : Ratio de Sortino (p√©nalise seulement downside)\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0 or current_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    instant_log_return = np.log(current_value / previous_value)\n",
    "    instant_log_return = np.clip(instant_log_return, -0.1, 0.1)\n",
    "    \n",
    "    WINDOW = 20\n",
    "    all_values_np = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    safe_values = np.where(all_values_np <= 0, 1e-9, all_values_np)\n",
    "    \n",
    "    if len(safe_values) > 1:\n",
    "        log_returns = np.diff(np.log(safe_values))\n",
    "    else:\n",
    "        log_returns = np.array([0.0])\n",
    "    \n",
    "    if len(log_returns) >= WINDOW:\n",
    "        negative_returns = log_returns[-WINDOW:][log_returns[-WINDOW:] < 0]\n",
    "        downside_vol = np.std(negative_returns) if len(negative_returns) > 0 else 0.001\n",
    "    else:\n",
    "        negative_returns = log_returns[log_returns < 0]\n",
    "        downside_vol = np.std(negative_returns) if len(negative_returns) > 0 else 0.001\n",
    "    \n",
    "    downside_vol = max(downside_vol, 0.001)\n",
    "    sortino = instant_log_return / (downside_vol * risk_penalty)\n",
    "    sortino = np.clip(sortino, -10, 10)\n",
    "    \n",
    "    return sortino * reward_scaling\n",
    "\n",
    "\n",
    "def reward_calmar_ratio(history, risk_penalty=0.2, reward_scaling=1.0) -> float:\n",
    "    \"\"\"REWARD 6 : Ratio de Calmar (rendement / max drawdown)\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    initial_value = 1000\n",
    "    total_return = (current_value - initial_value) / initial_value\n",
    "    \n",
    "    all_values = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    all_values = np.where(all_values <= 0, 1e-9, all_values)\n",
    "    \n",
    "    running_max = np.maximum.accumulate(all_values)\n",
    "    drawdowns = (all_values - running_max) / running_max\n",
    "    max_drawdown = abs(np.min(drawdowns)) if len(drawdowns) > 0 else 0.001\n",
    "    max_drawdown = max(max_drawdown, 0.01)\n",
    "    \n",
    "    calmar = total_return / (max_drawdown * risk_penalty)\n",
    "    calmar = np.clip(calmar, -10, 10)\n",
    "    \n",
    "    return calmar * reward_scaling\n",
    "\n",
    "\n",
    "def reward_hybrid_sharpe_momentum(history, risk_penalty=0.1, reward_scaling=1.5) -> float:\n",
    "    \"\"\"\n",
    "    REWARD HYBRIDE : Combine Sharpe + Momentum\n",
    "    Le meilleur des deux mondes !\n",
    "    \"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0 or current_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    # ========== PARTIE 1 : SHARPE ==========\n",
    "    instant_log_return = np.log(current_value / previous_value)\n",
    "    instant_log_return = np.clip(instant_log_return, -0.1, 0.1)\n",
    "    \n",
    "    WINDOW = 20\n",
    "    all_values_np = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    safe_values = np.where(all_values_np <= 0, 1e-9, all_values_np)\n",
    "    \n",
    "    if len(safe_values) > 1:\n",
    "        log_returns = np.diff(np.log(safe_values))\n",
    "    else:\n",
    "        log_returns = np.array([0.0])\n",
    "    \n",
    "    if len(log_returns) >= WINDOW:\n",
    "        volatility = np.std(log_returns[-WINDOW:])\n",
    "    else:\n",
    "        volatility = np.std(log_returns) if len(log_returns) > 1 else 1e-9\n",
    "    \n",
    "    volatility = max(volatility, 0.001)\n",
    "    sharpe_component = instant_log_return / (volatility * risk_penalty)\n",
    "    sharpe_component = np.clip(sharpe_component, -5, 5)  # Limiter √† ¬±5\n",
    "    \n",
    "    # ========== PARTIE 2 : MOMENTUM ==========\n",
    "    values = safe_values[-10:]\n",
    "    \n",
    "    if len(values) >= 5:\n",
    "        returns = np.diff(np.log(values))\n",
    "        momentum = np.mean(returns[-5:])\n",
    "        momentum_bonus = np.tanh(momentum * 10) * 0.5  # Bonus ¬±0.5 max\n",
    "    else:\n",
    "        momentum_bonus = 0\n",
    "    \n",
    "    # ========== COMBINAISON ==========\n",
    "    # 70% Sharpe + 30% Momentum\n",
    "    hybrid_reward = (0.7 * sharpe_component) + (0.3 * momentum_bonus * 10)\n",
    "    \n",
    "    return np.clip(hybrid_reward, -10, 10) * reward_scaling\n",
    "\n",
    "\n",
    "# Dictionnaire de toutes les fonctions\n",
    "REWARD_FUNCTIONS = {\n",
    "    \"simple_return\": reward_simple_return,\n",
    "    \"clipped_sharpe\": reward_clipped_sharpe,\n",
    "    \"momentum_based\": reward_momentum_based,\n",
    "    \"profit_drawdown\": reward_profit_drawdown,\n",
    "    \"sortino_ratio\": reward_sortino_ratio,\n",
    "    \"calmar_ratio\": reward_calmar_ratio,\n",
    "    \"hybrid_sharpe_momentum\" :reward_hybrid_sharpe_momentum\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ {len(REWARD_FUNCTIONS)} fonctions de r√©compense disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fonction d'Entra√Ænement avec M√©triques Compl√®tes WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonction d'entra√Ænement avec m√©triques compl√®tes pr√™te\n"
     ]
    }
   ],
   "source": [
    "def train_single_config(config, use_wandb=True):\n",
    "    \"\"\"Entra√Æne un seul mod√®le avec une configuration donn√©e\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ ENTRA√éNEMENT: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Extraire les param√®tres\n",
    "    algo = config['algo']\n",
    "    reward_type = config['reward_type']\n",
    "    risk_penalty = config['risk_penalty']\n",
    "    reward_scaling = config['reward_scaling']\n",
    "    learning_rate = config['learning_rate']\n",
    "    timesteps = config['timesteps']\n",
    "    \n",
    "    # Cr√©er la fonction de r√©compense\n",
    "    reward_fn = REWARD_FUNCTIONS[reward_type]\n",
    "    \n",
    "    def reward_wrapper(history):\n",
    "        return reward_fn(history, risk_penalty=risk_penalty, reward_scaling=reward_scaling)\n",
    "    \n",
    "    # Cr√©er l'environnement\n",
    "    env = gym.make(\n",
    "        \"MultiDatasetTradingEnv\",\n",
    "        dataset_dir=\"data/*.pkl\",\n",
    "        preprocess=preprocess,\n",
    "        portfolio_initial_value=1_000,\n",
    "        trading_fees=0.1/100,\n",
    "        borrow_interest_rate=0.02/100/24,\n",
    "        reward_function=reward_wrapper,\n",
    "    )\n",
    "    \n",
    "    env.add_metric('Portfolio Valuation', lambda h: round(h['portfolio_valuation', -1], 2))\n",
    "    \n",
    "    # Wrapping\n",
    "    log_dir = f\"models/{config['name']}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    env = Monitor(env, filename=os.path.join(log_dir, \"monitor.csv\"))\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    \n",
    "    # WandB avec TensorBoard\n",
    "    tensorboard_log_dir = f\"runs/{config['name']}\"\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=\"RL-project-trading\",\n",
    "            name=config['name'],\n",
    "            config=config,\n",
    "            reinit=True,\n",
    "            sync_tensorboard=True,  # Synchronise TensorBoard\n",
    "        )\n",
    "        print(f\"  üìä WandB: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/runs/{wandb.run.id}\")\n",
    "    \n",
    "    # Callback D√âTAILL√â avec TOUTES les m√©triques\n",
    "    class DetailedWandbCallback(BaseCallback):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.episode_rewards = []\n",
    "            self.episode_lengths = []\n",
    "            self.portfolio_values = []\n",
    "            self.episode_returns = []\n",
    "            self.max_drawdowns = []\n",
    "            \n",
    "        def _on_step(self):\n",
    "            # Logger tous les 100 steps\n",
    "            if use_wandb and self.n_calls % 100 == 0:\n",
    "                # M√©triques d'entra√Ænement de l'algorithme\n",
    "                if hasattr(self.model, 'logger') and self.model.logger:\n",
    "                    # Ces m√©triques viennent de l'algorithme lui-m√™me\n",
    "                    wandb.log({\n",
    "                        \"train/learning_rate\": self.model.learning_rate,\n",
    "                        \"timesteps\": self.num_timesteps,\n",
    "                    })\n",
    "            \n",
    "            # Logger les infos d'√©pisode (quand un √©pisode se termine)\n",
    "            for idx, info in enumerate(self.locals.get('infos', [])):\n",
    "                if 'episode' in info:\n",
    "                    episode_reward = info['episode']['r']\n",
    "                    episode_length = info['episode']['l']\n",
    "                    \n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.episode_lengths.append(episode_length)\n",
    "                    \n",
    "                    try:\n",
    "                        # R√©cup√©rer les infos d√©taill√©es de l'environnement\n",
    "                        base_env = self.training_env.envs[idx].unwrapped\n",
    "                        \n",
    "                        if hasattr(base_env, 'historical_info') and len(base_env.historical_info) > 0:\n",
    "                            # Portfolio value\n",
    "                            portfolio_value = base_env.historical_info[-1].get('portfolio_valuation', 1000)\n",
    "                            self.portfolio_values.append(portfolio_value)\n",
    "                            \n",
    "                            # Calcul du rendement\n",
    "                            total_return_pct = (portfolio_value - 1000) / 1000 * 100\n",
    "                            self.episode_returns.append(total_return_pct)\n",
    "                            \n",
    "                            # Calcul du max drawdown\n",
    "                            all_portfolio_values = [h['portfolio_valuation'] for h in base_env.historical_info]\n",
    "                            running_max = np.maximum.accumulate(all_portfolio_values)\n",
    "                            drawdowns = (np.array(all_portfolio_values) - running_max) / running_max\n",
    "                            max_drawdown = np.min(drawdowns) if len(drawdowns) > 0 else 0\n",
    "                            self.max_drawdowns.append(abs(max_drawdown) * 100)\n",
    "                            \n",
    "                            # Calcul de la volatilit√©\n",
    "                            if len(all_portfolio_values) > 1:\n",
    "                                returns = np.diff(np.log(all_portfolio_values))\n",
    "                                volatility = np.std(returns) * 100\n",
    "                            else:\n",
    "                                volatility = 0\n",
    "                            \n",
    "                            # Ratio de Sharpe r√©alis√© (approximatif)\n",
    "                            if volatility > 0:\n",
    "                                sharpe_ratio = total_return_pct / volatility\n",
    "                            else:\n",
    "                                sharpe_ratio = 0\n",
    "                            \n",
    "                            if use_wandb:\n",
    "                                # LOG COMPLET dans WandB\n",
    "                                wandb.log({\n",
    "                                    # √âpisode de base\n",
    "                                    \"episode/reward\": episode_reward,\n",
    "                                    \"episode/length\": episode_length,\n",
    "                                    \"episode/num_episodes\": len(self.episode_rewards),\n",
    "                                    \n",
    "                                    # Portfolio\n",
    "                                    \"episode/portfolio_value\": portfolio_value,\n",
    "                                    \"episode/total_return_pct\": total_return_pct,\n",
    "                                    \n",
    "                                    # Risque\n",
    "                                    \"episode/max_drawdown_pct\": abs(max_drawdown) * 100,\n",
    "                                    \"episode/volatility_pct\": volatility,\n",
    "                                    \"episode/sharpe_ratio\": sharpe_ratio,\n",
    "                                    \n",
    "                                    # Moyennes mobiles (importantes !)\n",
    "                                    \"episode/mean_reward_100\": np.mean(self.episode_rewards[-100:]),\n",
    "                                    \"episode/mean_portfolio_100\": np.mean(self.portfolio_values[-100:]),\n",
    "                                    \"episode/mean_return_100\": np.mean(self.episode_returns[-100:]),\n",
    "                                    \n",
    "                                    # M√©triques cumulatives\n",
    "                                    \"cumulative/total_episodes\": len(self.episode_rewards),\n",
    "                                    \"cumulative/best_portfolio\": max(self.portfolio_values),\n",
    "                                    \"cumulative/worst_portfolio\": min(self.portfolio_values),\n",
    "                                    \"cumulative/avg_episode_length\": np.mean(self.episode_lengths),\n",
    "                                    \n",
    "                                    # Timesteps\n",
    "                                    \"timesteps\": self.num_timesteps,\n",
    "                                })\n",
    "                            \n",
    "                            # Print console\n",
    "                            print(f\"  Episode {len(self.episode_rewards)}: \"\n",
    "                                  f\"Reward={episode_reward:.2f}, \"\n",
    "                                  f\"Portfolio=${portfolio_value:.2f} ({total_return_pct:+.1f}%), \"\n",
    "                                  f\"Drawdown={abs(max_drawdown)*100:.1f}%\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        # Fallback : logger au moins les rewards\n",
    "                        if use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"episode/reward\": episode_reward,\n",
    "                                \"episode/length\": episode_length,\n",
    "                                \"episode/num_episodes\": len(self.episode_rewards),\n",
    "                                \"episode/mean_reward_100\": np.mean(self.episode_rewards[-100:]),\n",
    "                                \"timesteps\": self.num_timesteps,\n",
    "                            })\n",
    "                        print(f\"  Episode {len(self.episode_rewards)}: Reward={episode_reward:.2f}\")\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    callback = DetailedWandbCallback()\n",
    "    \n",
    "    # Cr√©er le mod√®le AVEC tensorboard_log\n",
    "    if algo == \"PPO\":\n",
    "        model = PPO(\n",
    "            \"MlpPolicy\", vec_env,\n",
    "            learning_rate=config.get('learning_rate', 3e-4),\n",
    "            n_steps=config.get('n_steps', 2048),\n",
    "            batch_size=config.get('batch_size', 64),\n",
    "            n_epochs=config.get('n_epochs', 10),\n",
    "            gamma=config.get('gamma', 0.99),\n",
    "            gae_lambda=config.get('gae_lambda', 0.95),\n",
    "            clip_range=config.get('clip_range', 0.2),\n",
    "            ent_coef=config.get('ent_coef', 0.01),\n",
    "            vf_coef=config.get('vf_coef', 0.5),\n",
    "            verbose=0,\n",
    "            tensorboard_log=tensorboard_log_dir,\n",
    "        )\n",
    "    elif algo == \"SAC\":\n",
    "        model = SAC(\n",
    "            \"MlpPolicy\", vec_env,\n",
    "            learning_rate=learning_rate,\n",
    "            buffer_size=50000, batch_size=256,\n",
    "            gamma=0.99, tau=0.005,\n",
    "            verbose=0,\n",
    "            tensorboard_log=tensorboard_log_dir,\n",
    "        )\n",
    "   \n",
    "    # Entra√Æner\n",
    "    print(f\"  Timesteps: {timesteps}\")\n",
    "    print(f\"  Entra√Ænement en cours...\")\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    \n",
    "    # R√©sultats\n",
    "    results = {\n",
    "        \"name\": config['name'],\n",
    "        \"algo\": algo,\n",
    "        \"reward_type\": reward_type,\n",
    "        \"num_episodes\": len(callback.episode_rewards),\n",
    "        \"mean_reward\": np.mean(callback.episode_rewards) if callback.episode_rewards else 0,\n",
    "        \"final_portfolio\": callback.portfolio_values[-1] if callback.portfolio_values else 1000,\n",
    "        \"mean_portfolio\": np.mean(callback.portfolio_values) if callback.portfolio_values else 1000,\n",
    "        \"max_portfolio\": np.max(callback.portfolio_values) if callback.portfolio_values else 1000,\n",
    "        \"mean_return\": np.mean(callback.episode_returns) if callback.episode_returns else 0,\n",
    "        \"max_drawdown\": np.max(callback.max_drawdowns) if callback.max_drawdowns else 0,\n",
    "    }\n",
    "    \n",
    "    # Logging final dans WandB\n",
    "    if use_wandb and len(callback.episode_rewards) > 0:\n",
    "        wandb.log({\n",
    "            \"final/total_episodes\": len(callback.episode_rewards),\n",
    "            \"final/mean_reward\": results[\"mean_reward\"],\n",
    "            \"final/portfolio_value\": results[\"final_portfolio\"],\n",
    "            \"final/mean_portfolio\": results[\"mean_portfolio\"],\n",
    "            \"final/max_portfolio\": results[\"max_portfolio\"],\n",
    "            \"final/total_return_pct\": (results[\"final_portfolio\"] - 1000) / 10,\n",
    "            \"final/mean_return_pct\": results[\"mean_return\"],\n",
    "            \"final/max_drawdown_pct\": results[\"max_drawdown\"],\n",
    "        })\n",
    "        \n",
    "        # Cr√©er un graphique r√©capitulatif custom\n",
    "        if len(callback.portfolio_values) > 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Portfolio evolution\n",
    "            ax1.plot(callback.portfolio_values, linewidth=2, color='green', alpha=0.7)\n",
    "            ax1.axhline(y=1000, color='red', linestyle='--', alpha=0.5, label='Initial')\n",
    "            ax1.fill_between(range(len(callback.portfolio_values)), \n",
    "                            1000, callback.portfolio_values, \n",
    "                            alpha=0.3, color='green' if callback.portfolio_values[-1] > 1000 else 'red')\n",
    "            ax1.set_title(f'Portfolio Evolution - {config[\"name\"]}')\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Portfolio Value ($)')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.legend()\n",
    "            \n",
    "            # Rewards evolution\n",
    "            ax2.plot(callback.episode_rewards, alpha=0.3, color='blue', label='Raw')\n",
    "            if len(callback.episode_rewards) > 10:\n",
    "                window = min(20, len(callback.episode_rewards) // 5)\n",
    "                smoothed = pd.Series(callback.episode_rewards).rolling(window=window).mean()\n",
    "                ax2.plot(smoothed, linewidth=2, color='darkblue', label=f'Smoothed ({window})')\n",
    "            ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "            ax2.set_title('Episode Rewards')\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Reward')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            wandb.log({f\"charts/{config['name']}_summary\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "        \n",
    "        wandb.finish()\n",
    "    \n",
    "    # Sauvegarder\n",
    "    model.save(os.path.join(log_dir, \"model.zip\"))\n",
    "    vec_env.save(os.path.join(log_dir, \"vec_normalize.pkl\"))\n",
    "    \n",
    "    # Nettoyer\n",
    "    vec_env.close()\n",
    "    \n",
    "    print(f\"  ‚úÖ Portfolio Final: ${results['final_portfolio']:.2f} \"\n",
    "          f\"({(results['final_portfolio']-1000)/10:+.1f}%)\")\n",
    "    print(f\"  ‚úÖ Max Drawdown: {results['max_drawdown']:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Fonction d'entra√Ænement avec m√©triques compl√®tes pr√™te\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancer tous les tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ ENTRA√éNEMENT: sharpe_balanced_v2\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/gueguenoucedric/Documents/RL-projet-trading/wandb/run-20251223_142524-78w53a07</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laetitia-montbarbon-cpe-lyon/RL-project-trading/runs/78w53a07' target=\"_blank\">sharpe_balanced_v2</a></strong> to <a href='https://wandb.ai/laetitia-montbarbon-cpe-lyon/RL-project-trading' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laetitia-montbarbon-cpe-lyon/RL-project-trading' target=\"_blank\">https://wandb.ai/laetitia-montbarbon-cpe-lyon/RL-project-trading</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laetitia-montbarbon-cpe-lyon/RL-project-trading/runs/78w53a07' target=\"_blank\">https://wandb.ai/laetitia-montbarbon-cpe-lyon/RL-project-trading/runs/78w53a07</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìä WandB: https://wandb.ai/laetitia-montbarbon-cpe-lyon/RL-project-trading/runs/78w53a07\n",
      "  Timesteps: 500000\n",
      "  Entra√Ænement en cours...\n",
      "Market Return : 39.94%   |   Portfolio Return : -97.53%   |   Portfolio Valuation : 24.65   |   \n",
      "  Episode 1: Reward=-11425.20, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return :  5.27%   |   Portfolio Return : -99.99%   |   Portfolio Valuation : 0.09   |   \n",
      "  Episode 2: Reward=-95605.31, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 163.91%   |   Portfolio Return : -100.00%   |   Portfolio Valuation : 0.0   |   \n",
      "  Episode 3: Reward=-21739.79, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 10.54%   |   Portfolio Return : -67.60%   |   Portfolio Valuation : 324.02   |   \n",
      "  Episode 4: Reward=-4955.58, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 103.26%   |   Portfolio Return : -85.17%   |   Portfolio Valuation : 148.33   |   \n",
      "  Episode 5: Reward=-11672.79, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 44.45%   |   Portfolio Return : -13.99%   |   Portfolio Valuation : 860.14   |   \n",
      "  Episode 6: Reward=-1100.58, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 27.70%   |   Portfolio Return : -16.33%   |   Portfolio Valuation : 836.67   |   \n",
      "  Episode 7: Reward=-1442.94, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 904.05%   |   Portfolio Return : -97.86%   |   Portfolio Valuation : 21.35   |   \n",
      "  Episode 8: Reward=-3718.60, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 27.70%   |   Portfolio Return : 33.52%   |   Portfolio Valuation : 1335.18   |   \n",
      "  Episode 9: Reward=314.45, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 904.05%   |   Portfolio Return : -49.73%   |   Portfolio Valuation : 502.66   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.FileIO name='/Users/gueguenoucedric/Documents/RL-projet-trading/models/sharpe_balanced_v2/monitor.csv' mode='wb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gueguenoucedric/Documents/RL-projet-trading/.venv/lib/python3.13/site-packages/pandas/core/arrays/datetimelike.py\", line 477, in astype\n",
      "    converted = ints_to_pydatetime(\n",
      "ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/gueguenoucedric/Documents/RL-projet-trading/models/sharpe_balanced_v2/monitor.csv' mode='wt' encoding='UTF-8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 10: Reward=-1464.66, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 103.26%   |   Portfolio Return : -30.59%   |   Portfolio Valuation : 694.1   |   \n",
      "  Episode 11: Reward=-2797.45, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 10.54%   |   Portfolio Return : 45.00%   |   Portfolio Valuation : 1449.99   |   \n",
      "  Episode 12: Reward=370.39, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 677.81%   |   Portfolio Return : -82.49%   |   Portfolio Valuation : 175.14   |   \n",
      "  Episode 13: Reward=-235.15, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 44.45%   |   Portfolio Return : 63.32%   |   Portfolio Valuation : 1633.19   |   \n",
      "  Episode 14: Reward=272.81, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 39.94%   |   Portfolio Return : -18.32%   |   Portfolio Valuation : 816.79   |   \n",
      "  Episode 15: Reward=-270.86, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return :  5.27%   |   Portfolio Return : -51.41%   |   Portfolio Valuation : 485.88   |   \n",
      "  Episode 16: Reward=-5290.75, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 606.29%   |   Portfolio Return : -104.25%   |   Portfolio Valuation : -42.51   |   \n",
      "  Episode 17: Reward=4726.88, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 677.81%   |   Portfolio Return : -82.13%   |   Portfolio Valuation : 178.73   |   \n",
      "  Episode 18: Reward=-428.88, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 904.05%   |   Portfolio Return : 629.87%   |   Portfolio Valuation : 7298.69   |   \n",
      "  Episode 19: Reward=2698.66, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 39.94%   |   Portfolio Return : -18.22%   |   Portfolio Valuation : 817.78   |   \n",
      "  Episode 20: Reward=-512.57, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 10.54%   |   Portfolio Return : 18.47%   |   Portfolio Valuation : 1184.75   |   \n",
      "  Episode 21: Reward=220.72, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 103.26%   |   Portfolio Return : -16.01%   |   Portfolio Valuation : 839.91   |   \n",
      "  Episode 22: Reward=-1110.52, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 44.45%   |   Portfolio Return : -6.45%   |   Portfolio Valuation : 935.54   |   \n",
      "  Episode 23: Reward=-56.02, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 343.52%   |   Portfolio Return : -109.97%   |   Portfolio Valuation : -99.74   |   \n",
      "  Episode 24: Reward=14.04, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return : 27.70%   |   Portfolio Return : 43.01%   |   Portfolio Valuation : 1430.07   |   \n",
      "  Episode 25: Reward=707.87, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return :  5.27%   |   Portfolio Return : -36.16%   |   Portfolio Valuation : 638.37   |   \n",
      "  Episode 26: Reward=-3364.54, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n",
      "Market Return :  5.27%   |   Portfolio Return : -32.86%   |   Portfolio Valuation : 671.44   |   \n",
      "  Episode 27: Reward=-2377.97, Portfolio=$1000.00 (+0.0%), Drawdown=0.0%\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for config in TEST_CONFIGS:\n",
    "    try:\n",
    "        result = train_single_config(config, use_wandb=USE_WANDB)\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur avec {config['name']}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n‚úÖ {len(all_results)} configurations test√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test et Comparaison de TOUS les Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üß™ TEST DE PORTFOLIO FINAL - TOUS LES MOD√àLES\n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Trouv√© 16 mod√®les √† tester\n",
      "\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[1/16] üîç Test de : aggressive_ppo_20251222_185706\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚ùå Erreur : spaces must have the same shape: (25,) != (13,)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[2/16] üîç Test de : hybrid_sharpe_momentum\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/7n/j303573n64g3f_lsp13bx1vh0000gn/T/ipykernel_88117/1072494889.py\", line 95, in <module>\n",
      "    test_vec_env = VecNormalize.load(vec_normalize_file, test_vec_env)\n",
      "  File \"/Users/gueguenoucedric/Documents/RL-projet-trading/.venv/lib/python3.13/site-packages/stable_baselines3/common/vec_env/vec_normalize.py\", line 321, in load\n",
      "    vec_normalize.set_venv(venv)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/Users/gueguenoucedric/Documents/RL-projet-trading/.venv/lib/python3.13/site-packages/stable_baselines3/common/vec_env/vec_normalize.py\", line 171, in set_venv\n",
      "    utils.check_shape_equal(self.observation_space, venv.observation_space)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gueguenoucedric/Documents/RL-projet-trading/.venv/lib/python3.13/site-packages/stable_baselines3/common/utils.py\", line 339, in check_shape_equal\n",
      "    assert space1.shape == space2.shape, f\"spaces must have the same shape: {space1.shape} != {space2.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: spaces must have the same shape: (25,) != (13,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 103.26%   |   Portfolio Return : 49.66%   |   Portfolio Valuation : 1496.58   |   \n",
      "      √âpisode 1: $1496.58\n",
      "Market Return : 343.52%   |   Portfolio Return : -107.72%   |   Portfolio Valuation : -77.2   |   \n",
      "Market Return : 904.05%   |   Portfolio Return : 290.92%   |   Portfolio Valuation : 3909.18   |   \n",
      "Market Return : 44.45%   |   Portfolio Return :  9.62%   |   Portfolio Valuation : 1096.19   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : 57.98%   |   Portfolio Valuation : 1579.76   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : 51.74%   |   Portfolio Valuation : 1517.44   |   \n",
      "Market Return : 343.52%   |   Portfolio Return : -118.61%   |   Portfolio Valuation : -186.13   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : 142.79%   |   Portfolio Valuation : 2427.95   |   \n",
      "Market Return : 904.05%   |   Portfolio Return : 330.82%   |   Portfolio Valuation : 4308.24   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -11.80%   |   Portfolio Valuation : 882.03   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $1695.40 ¬± $1416.05\n",
      "      Rendement Moyen    : +69.54% ¬± 141.60%\n",
      "      Min - Max          : $-186.13 - $4308.24\n",
      "      M√©diane            : $1507.01\n",
      "      Taux de R√©ussite   : 70%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[3/16] üîç Test de : momentum_aggressive\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 103.26%   |   Portfolio Return : -27.29%   |   Portfolio Valuation : 727.1   |   \n",
      "      √âpisode 1: $727.10\n",
      "Market Return : 10.54%   |   Portfolio Return :  7.75%   |   Portfolio Valuation : 1077.5   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : -1.01%   |   Portfolio Valuation : 989.88   |   \n",
      "Market Return : 343.52%   |   Portfolio Return : -121.30%   |   Portfolio Valuation : -212.98   |   \n",
      "Market Return : 904.05%   |   Portfolio Return : -89.09%   |   Portfolio Valuation : 109.11   |   \n",
      "Market Return : 343.52%   |   Portfolio Return : -121.35%   |   Portfolio Valuation : -213.47   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : 37.72%   |   Portfolio Valuation : 1377.2   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -86.45%   |   Portfolio Valuation : 135.5   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : -10.13%   |   Portfolio Valuation : 898.73   |   \n",
      "Market Return : 904.05%   |   Portfolio Return : -89.08%   |   Portfolio Valuation : 109.17   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $499.77 ¬± $548.57\n",
      "      Rendement Moyen    : -50.02% ¬± 54.86%\n",
      "      Min - Max          : $-213.47 - $1377.20\n",
      "      M√©diane            : $431.30\n",
      "      Taux de R√©ussite   : 20%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[4/16] üîç Test de : ppo_aggressive_sharpe\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 44.45%   |   Portfolio Return : -46.84%   |   Portfolio Valuation : 531.58   |   \n",
      "      √âpisode 1: $531.58\n",
      "Market Return : 163.91%   |   Portfolio Return : 69.59%   |   Portfolio Valuation : 1695.89   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -93.35%   |   Portfolio Valuation : 66.46   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : -45.34%   |   Portfolio Valuation : 546.61   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : -45.26%   |   Portfolio Valuation : 547.39   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : -58.22%   |   Portfolio Valuation : 417.77   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -99.97%   |   Portfolio Valuation : 0.31   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : -59.45%   |   Portfolio Valuation : 405.51   |   \n",
      "Market Return : 163.91%   |   Portfolio Return : 59.95%   |   Portfolio Valuation : 1599.45   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : -58.19%   |   Portfolio Valuation : 418.14   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $622.91 ¬± $543.45\n",
      "      Rendement Moyen    : -37.71% ¬± 54.34%\n",
      "      Min - Max          : $0.31 - $1695.89\n",
      "      M√©diane            : $474.86\n",
      "      Taux de R√©ussite   : 20%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[5/16] üîç Test de : profit_with_drawdown\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 39.94%   |   Portfolio Return : -14.06%   |   Portfolio Valuation : 859.39   |   \n",
      "      √âpisode 1: $859.39\n",
      "Market Return : 27.70%   |   Portfolio Return : -22.53%   |   Portfolio Valuation : 774.74   |   \n",
      "Market Return : 163.91%   |   Portfolio Return : -98.26%   |   Portfolio Valuation : 17.38   |   \n",
      "Market Return : 44.45%   |   Portfolio Return : -20.97%   |   Portfolio Valuation : 790.34   |   \n",
      "Market Return : 44.45%   |   Portfolio Return : -21.01%   |   Portfolio Valuation : 789.88   |   \n",
      "Market Return : 163.91%   |   Portfolio Return : -98.42%   |   Portfolio Valuation : 15.8   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : -15.72%   |   Portfolio Valuation : 842.85   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -74.25%   |   Portfolio Valuation : 257.48   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : -25.86%   |   Portfolio Valuation : 741.45   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -98.77%   |   Portfolio Valuation : 12.26   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $510.16 ¬± $362.20\n",
      "      Rendement Moyen    : -48.98% ¬± 36.22%\n",
      "      Min - Max          : $12.26 - $859.39\n",
      "      M√©diane            : $758.10\n",
      "      Taux de R√©ussite   : 0%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[6/16] üîç Test de : sac_sharpe\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : SAC\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 39.94%   |   Portfolio Return :  4.60%   |   Portfolio Valuation : 1046.04   |   \n",
      "      √âpisode 1: $1046.04\n",
      "Market Return : 904.05%   |   Portfolio Return : -21.58%   |   Portfolio Valuation : 784.2   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -3.71%   |   Portfolio Valuation : 962.85   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -60.66%   |   Portfolio Valuation : 393.42   |   \n",
      "Market Return : 163.91%   |   Portfolio Return : -99.76%   |   Portfolio Valuation : 2.4   |   \n",
      "Market Return : 27.70%   |   Portfolio Return :  7.16%   |   Portfolio Valuation : 1071.6   |   \n",
      "Market Return : 44.45%   |   Portfolio Return : 37.95%   |   Portfolio Valuation : 1379.53   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : -5.43%   |   Portfolio Valuation : 945.72   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -72.66%   |   Portfolio Valuation : 273.35   |   \n",
      "Market Return : 904.05%   |   Portfolio Return : -21.09%   |   Portfolio Valuation : 789.05   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $764.82 ¬± $398.09\n",
      "      Rendement Moyen    : -23.52% ¬± 39.81%\n",
      "      Min - Max          : $2.40 - $1379.53\n",
      "      M√©diane            : $867.38\n",
      "      Taux de R√©ussite   : 30%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[7/16] üîç Test de : sac_simple\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : SAC\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 103.26%   |   Portfolio Return : -89.18%   |   Portfolio Valuation : 108.18   |   \n",
      "      √âpisode 1: $108.18\n",
      "Market Return : 44.45%   |   Portfolio Return : -20.47%   |   Portfolio Valuation : 795.34   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 10.81%   |   Portfolio Valuation : 1108.1   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -97.14%   |   Portfolio Valuation : 28.6   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -88.45%   |   Portfolio Valuation : 115.53   |   \n",
      "Market Return : 44.45%   |   Portfolio Return : -20.29%   |   Portfolio Valuation : 797.06   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -89.18%   |   Portfolio Valuation : 108.25   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 10.28%   |   Portfolio Valuation : 1102.82   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -97.16%   |   Portfolio Valuation : 28.45   |   \n",
      "Market Return : 163.91%   |   Portfolio Return : 198.16%   |   Portfolio Valuation : 2981.6   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $717.39 ¬± $865.67\n",
      "      Rendement Moyen    : -28.26% ¬± 86.57%\n",
      "      Min - Max          : $28.45 - $2981.60\n",
      "      M√©diane            : $455.44\n",
      "      Taux de R√©ussite   : 30%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[8/16] üîç Test de : sharpe_aggressive\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 44.45%   |   Portfolio Return : 38.25%   |   Portfolio Valuation : 1382.54   |   \n",
      "      √âpisode 1: $1382.54\n",
      "Market Return : 677.81%   |   Portfolio Return : -97.69%   |   Portfolio Valuation : 23.11   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 85.99%   |   Portfolio Valuation : 1859.9   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -65.98%   |   Portfolio Valuation : 340.18   |   \n",
      "Market Return : 904.05%   |   Portfolio Return : -93.15%   |   Portfolio Valuation : 68.48   |   \n",
      "Market Return : 911.58%   |   Portfolio Return : -109.23%   |   Portfolio Valuation : -92.33   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 81.74%   |   Portfolio Valuation : 1817.36   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -66.03%   |   Portfolio Valuation : 339.7   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : -25.65%   |   Portfolio Valuation : 743.54   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -65.98%   |   Portfolio Valuation : 340.23   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $682.27 ¬± $702.28\n",
      "      Rendement Moyen    : -31.77% ¬± 70.23%\n",
      "      Min - Max          : $-92.33 - $1859.90\n",
      "      M√©diane            : $340.21\n",
      "      Taux de R√©ussite   : 30%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[9/16] üîç Test de : sharpe_aggressive_v2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : -57.42%   |   Portfolio Return : -114.83%   |   Portfolio Valuation : -148.25   |   \n",
      "      √âpisode 1: $-148.25\n",
      "Market Return : 10.54%   |   Portfolio Return : 16.05%   |   Portfolio Valuation : 1160.54   |   \n",
      "Market Return : 39.94%   |   Portfolio Return :  9.32%   |   Portfolio Valuation : 1093.17   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -76.08%   |   Portfolio Valuation : 239.22   |   \n",
      "Market Return : 44.45%   |   Portfolio Return : 23.23%   |   Portfolio Valuation : 1232.3   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : 26.56%   |   Portfolio Valuation : 1265.59   |   \n",
      "Market Return : -57.42%   |   Portfolio Return : -114.83%   |   Portfolio Valuation : -148.28   |   \n",
      "Market Return : 39.94%   |   Portfolio Return :  1.77%   |   Portfolio Valuation : 1017.69   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -12.20%   |   Portfolio Valuation : 877.95   |   \n",
      "Market Return : 39.94%   |   Portfolio Return :  2.42%   |   Portfolio Valuation : 1024.22   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $761.42 ¬± $531.16\n",
      "      Rendement Moyen    : -23.86% ¬± 53.12%\n",
      "      Min - Max          : $-148.28 - $1265.59\n",
      "      M√©diane            : $1020.96\n",
      "      Taux de R√©ussite   : 60%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[10/16] üîç Test de : sharpe_balanced\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 44.45%   |   Portfolio Return : 84.25%   |   Portfolio Valuation : 1842.45   |   \n",
      "      √âpisode 1: $1842.45\n",
      "Market Return : -60.53%   |   Portfolio Return : -103.76%   |   Portfolio Valuation : -37.6   |   \n",
      "Market Return : 103.57%   |   Portfolio Return : -102.74%   |   Portfolio Valuation : -27.36   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : 46.80%   |   Portfolio Valuation : 1467.98   |   \n",
      "Market Return : 828.53%   |   Portfolio Return : -132.28%   |   Portfolio Valuation : -322.77   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : 280.27%   |   Portfolio Valuation : 3802.72   |   \n",
      "Market Return : 138.60%   |   Portfolio Return : -128.96%   |   Portfolio Valuation : -289.63   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -10.12%   |   Portfolio Valuation : 898.79   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : 49.88%   |   Portfolio Valuation : 1498.8   |   \n",
      "Market Return : 138.60%   |   Portfolio Return : -130.70%   |   Portfolio Valuation : -307.03   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $852.63 ¬± $1268.03\n",
      "      Rendement Moyen    : -14.74% ¬± 126.80%\n",
      "      Min - Max          : $-322.77 - $3802.72\n",
      "      M√©diane            : $435.71\n",
      "      Taux de R√©ussite   : 40%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[11/16] üîç Test de : sharpe_balanced_v2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return :  5.27%   |   Portfolio Return : -24.03%   |   Portfolio Valuation : 759.74   |   \n",
      "      √âpisode 1: $759.74\n",
      "Market Return : 44.45%   |   Portfolio Return : 71.86%   |   Portfolio Valuation : 1718.56   |   \n",
      "Market Return : 103.26%   |   Portfolio Return :  8.78%   |   Portfolio Valuation : 1087.79   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 145.00%   |   Portfolio Valuation : 2449.96   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -77.19%   |   Portfolio Valuation : 228.1   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : 71.24%   |   Portfolio Valuation : 1712.44   |   \n",
      "Market Return : 18.80%   |   Portfolio Return : -112.63%   |   Portfolio Valuation : -126.27   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 144.93%   |   Portfolio Valuation : 2449.26   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -26.19%   |   Portfolio Valuation : 738.09   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -73.31%   |   Portfolio Valuation : 266.86   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $1128.45 ¬± $871.63\n",
      "      Rendement Moyen    : +12.85% ¬± 87.16%\n",
      "      Min - Max          : $-126.27 - $2449.96\n",
      "      M√©diane            : $923.76\n",
      "      Taux de R√©ussite   : 50%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[12/16] üîç Test de : sharpe_conservative\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 10.54%   |   Portfolio Return : 62.66%   |   Portfolio Valuation : 1626.56   |   \n",
      "      √âpisode 1: $1626.56\n",
      "Market Return : -57.42%   |   Portfolio Return : -114.82%   |   Portfolio Valuation : -148.24   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : 1735.67%   |   Portfolio Valuation : 18356.68   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : 26.68%   |   Portfolio Valuation : 1266.79   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : 55.36%   |   Portfolio Valuation : 1553.61   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 150.38%   |   Portfolio Valuation : 2503.82   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -8.17%   |   Portfolio Valuation : 918.32   |   \n",
      "Market Return : 112.03%   |   Portfolio Return : -116.46%   |   Portfolio Valuation : -164.6   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : 21.66%   |   Portfolio Valuation : 1216.59   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : 1420.23%   |   Portfolio Valuation : 15202.34   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $4233.19 ¬± $6357.56\n",
      "      Rendement Moyen    : +323.32% ¬± 635.76%\n",
      "      Min - Max          : $-164.60 - $18356.68\n",
      "      M√©diane            : $1410.20\n",
      "      Taux de R√©ussite   : 70%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[13/16] üîç Test de : sharpe_conservative_v2\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 216.38%   |   Portfolio Return : -107.61%   |   Portfolio Valuation : -76.09   |   \n",
      "      √âpisode 1: $-76.09\n",
      "Market Return : 44.45%   |   Portfolio Return : -23.03%   |   Portfolio Valuation : 769.71   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : 22.33%   |   Portfolio Valuation : 1223.28   |   \n",
      "Market Return : 90.91%   |   Portfolio Return : -104.90%   |   Portfolio Valuation : -49.01   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -95.73%   |   Portfolio Valuation : 42.7   |   \n",
      "Market Return : 44.45%   |   Portfolio Return :  0.60%   |   Portfolio Valuation : 1005.97   |   \n",
      "Market Return : 111.29%   |   Portfolio Return : -100.68%   |   Portfolio Valuation : -6.79   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -54.29%   |   Portfolio Valuation : 457.13   |   \n",
      "Market Return : 949.74%   |   Portfolio Return : -100.34%   |   Portfolio Valuation : -3.41   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 25.70%   |   Portfolio Valuation : 1257.02   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $462.05 ¬± $525.84\n",
      "      Rendement Moyen    : -53.79% ¬± 52.58%\n",
      "      Min - Max          : $-76.09 - $1257.02\n",
      "      M√©diane            : $249.91\n",
      "      Taux de R√©ussite   : 30%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[14/16] üîç Test de : simple_return_baseline\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚ö†Ô∏è Config non trouv√©e, utilisation de simple_return\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 974.03%   |   Portfolio Return : -133.00%   |   Portfolio Valuation : -330.0   |   \n",
      "      √âpisode 1: $-330.00\n",
      "Market Return : 10.54%   |   Portfolio Return : -10.05%   |   Portfolio Valuation : 899.54   |   \n",
      "Market Return : 44.45%   |   Portfolio Return : 21.65%   |   Portfolio Valuation : 1216.47   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -33.27%   |   Portfolio Valuation : 667.28   |   \n",
      "Market Return : 27.70%   |   Portfolio Return : -7.86%   |   Portfolio Valuation : 921.36   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -33.18%   |   Portfolio Valuation : 668.21   |   \n",
      "Market Return : 974.03%   |   Portfolio Return : -134.13%   |   Portfolio Valuation : -341.29   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : 272.84%   |   Portfolio Valuation : 3728.41   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : -46.23%   |   Portfolio Valuation : 537.73   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 55.02%   |   Portfolio Valuation : 1550.21   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $951.79 ¬± $1085.78\n",
      "      Rendement Moyen    : -4.82% ¬± 108.58%\n",
      "      Min - Max          : $-341.29 - $3728.41\n",
      "      M√©diane            : $783.88\n",
      "      Taux de R√©ussite   : 30%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[15/16] üîç Test de : simple_return_optimized\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 39.94%   |   Portfolio Return : 67.18%   |   Portfolio Valuation : 1671.82   |   \n",
      "      √âpisode 1: $1671.82\n",
      "Market Return : 44.45%   |   Portfolio Return :  4.59%   |   Portfolio Valuation : 1045.89   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : 25.31%   |   Portfolio Valuation : 1253.05   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -39.17%   |   Portfolio Valuation : 608.34   |   \n",
      "Market Return : 949.74%   |   Portfolio Return : -102.61%   |   Portfolio Valuation : -26.08   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -56.38%   |   Portfolio Valuation : 436.24   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -81.10%   |   Portfolio Valuation : 189.01   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 23.29%   |   Portfolio Valuation : 1232.91   |   \n",
      "Market Return : 44.45%   |   Portfolio Return : -0.25%   |   Portfolio Valuation : 997.48   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : -80.46%   |   Portfolio Valuation : 195.43   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $760.41 ¬± $531.68\n",
      "      Rendement Moyen    : -23.96% ¬± 53.17%\n",
      "      Min - Max          : $-26.08 - $1671.82\n",
      "      M√©diane            : $802.91\n",
      "      Taux de R√©ussite   : 40%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "[16/16] üîç Test de : sortino_balanced\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "   üì¶ Algorithme : PPO\n",
      "   ‚úÖ Mod√®le charg√©\n",
      "   ‚úÖ Environnement cr√©√© et normalis√©\n",
      "   üéÆ Test sur 10 √©pisodes...\n",
      "Market Return : 117.56%   |   Portfolio Return : -101.67%   |   Portfolio Valuation : -16.68   |   \n",
      "      √âpisode 1: $-16.68\n",
      "Market Return : 828.53%   |   Portfolio Return : -466.51%   |   Portfolio Valuation : -3665.1   |   \n",
      "Market Return : 10.54%   |   Portfolio Return : 28.94%   |   Portfolio Valuation : 1289.44   |   \n",
      "Market Return :  5.27%   |   Portfolio Return : -13.05%   |   Portfolio Valuation : 869.5   |   \n",
      "Market Return : 117.56%   |   Portfolio Return : -101.73%   |   Portfolio Valuation : -17.25   |   \n",
      "Market Return : 39.94%   |   Portfolio Return : 116.27%   |   Portfolio Valuation : 2162.72   |   \n",
      "Market Return : 44.45%   |   Portfolio Return : 73.30%   |   Portfolio Valuation : 1732.96   |   \n",
      "Market Return : 677.81%   |   Portfolio Return : 72.79%   |   Portfolio Valuation : 1727.86   |   \n",
      "Market Return : 103.26%   |   Portfolio Return : 45.32%   |   Portfolio Valuation : 1453.22   |   \n",
      "Market Return : 828.53%   |   Portfolio Return : -433.10%   |   Portfolio Valuation : -3330.95   |   \n",
      "\n",
      "   üìä R√âSULTATS (10/10 √©pisodes):\n",
      "      Portfolio Moyen    : $220.57 ¬± $1980.88\n",
      "      Rendement Moyen    : -77.94% ¬± 198.09%\n",
      "      Min - Max          : $-3665.10 - $2162.72\n",
      "      M√©diane            : $1079.47\n",
      "      Taux de R√©ussite   : 50%\n",
      "   ‚úÖ Test r√©ussi\n",
      "\n",
      "====================================================================================================\n",
      "‚úÖ Tests termin√©s : 15/16 mod√®les test√©s avec succ√®s\n",
      "\n",
      "‚ö†Ô∏è 1 mod√®les ont √©chou√©:\n",
      "   - aggressive_ppo_20251222_185706: spaces must have the same shape: (25,) != (13,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TEST DE PORTFOLIO FINAL - TOUS LES MOD√àLES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1. Trouver tous les mod√®les\n",
    "model_dirs = sorted(glob.glob(\"models/*/\"))\n",
    "print(f\"\\n‚úÖ Trouv√© {len(model_dirs)} mod√®les √† tester\\n\")\n",
    "\n",
    "# 2. Stocker les r√©sultats\n",
    "portfolio_results = []\n",
    "failed_models = []\n",
    "\n",
    "# 3. Tester chaque mod√®le\n",
    "for idx, model_dir in enumerate(model_dirs, 1):\n",
    "    model_name = os.path.basename(model_dir.rstrip('/'))\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*100}\")\n",
    "    print(f\"[{idx}/{len(model_dirs)}] Test de : {model_name}\")\n",
    "    print(f\"{'‚îÄ'*100}\")\n",
    "    \n",
    "    try:\n",
    "        # V√©rifier que les fichiers existent\n",
    "        model_file = os.path.join(model_dir, \"model.zip\")\n",
    "        vec_normalize_file = os.path.join(model_dir, \"vec_normalize.pkl\")\n",
    "        \n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"   Mod√®le introuvable\")\n",
    "            failed_models.append({'name': model_name, 'reason': 'Model file not found'})\n",
    "            continue\n",
    "        \n",
    "        if not os.path.exists(vec_normalize_file):\n",
    "            print(f\"   vec_normalize.pkl introuvable\")\n",
    "            failed_models.append({'name': model_name, 'reason': 'vec_normalize.pkl not found'})\n",
    "            continue\n",
    "        \n",
    "        # D√©tecter l'algorithme\n",
    "        algo = None\n",
    "        if 'sac' in model_name.lower():\n",
    "            algo = 'SAC'\n",
    "        else:\n",
    "            algo = 'PPO'\n",
    "        \n",
    "        print(f\"   üì¶ Algorithme : {algo}\")\n",
    "        \n",
    "        # Charger le mod√®le\n",
    "        if algo == 'PPO':\n",
    "            loaded_model = PPO.load(model_file)\n",
    "        elif algo == 'SAC':\n",
    "            loaded_model = SAC.load(model_file)\n",
    "       \n",
    "        print(f\"‚úÖ Mod√®le charg√©\")\n",
    "        \n",
    "        # Trouver la config correspondante\n",
    "        config = None\n",
    "        for test_config in TEST_CONFIGS:\n",
    "            if test_config['name'] == model_name:\n",
    "                config = test_config\n",
    "                break\n",
    "        \n",
    "        # Si pas de config, utiliser simple_return\n",
    "        if config is None:\n",
    "            print(f\"   Config non trouv√©e, utilisation de simple_return\")\n",
    "            reward_fn = reward_simple_return\n",
    "            def reward_wrapper(history):\n",
    "                return reward_fn(history)\n",
    "        else:\n",
    "            reward_fn = REWARD_FUNCTIONS[config['reward_type']]\n",
    "            def reward_wrapper(history):\n",
    "                return reward_fn(history, \n",
    "                               risk_penalty=config.get('risk_penalty', 0.1), \n",
    "                               reward_scaling=config.get('reward_scaling', 1.0))\n",
    "        \n",
    "        # Cr√©er un environnement de test (comme dans votre code original)\n",
    "        test_env = gym.make(\n",
    "            \"MultiDatasetTradingEnv\",\n",
    "            dataset_dir=\"data/*.pkl\",\n",
    "            preprocess=preprocess,\n",
    "            portfolio_initial_value=1_000,\n",
    "            trading_fees=0.1/100,\n",
    "            borrow_interest_rate=0.02/100/24,\n",
    "            reward_function=reward_wrapper,\n",
    "        )\n",
    "        \n",
    "        test_env.add_metric('Portfolio Valuation', lambda h: round(h['portfolio_valuation', -1], 2))\n",
    "        \n",
    "        # Wrapping (EXACTEMENT comme votre code)\n",
    "        test_env = Monitor(test_env)\n",
    "        test_vec_env = DummyVecEnv([lambda: test_env])\n",
    "        \n",
    "        # Charger la normalisation COMPL√àTE\n",
    "        test_vec_env = VecNormalize.load(vec_normalize_file, test_vec_env)\n",
    "        \n",
    "        print(f\"‚úÖ Environnement cr√©√© et normalis√©\")\n",
    "        \n",
    "        # Tester sur 10 √©pisodes\n",
    "        print(f\"Test sur 10 √©pisodes...\")\n",
    "        \n",
    "        episode_portfolios = []\n",
    "        episode_returns = []\n",
    "        \n",
    "        for ep in range(10):\n",
    "            try:\n",
    "                # EXACTEMENT VOTRE CODE DE LA CELL 9\n",
    "                obs = test_vec_env.reset()\n",
    "                done = False\n",
    "                final_info = None\n",
    "                \n",
    "                while not done:\n",
    "                    action, _ = loaded_model.predict(obs, deterministic=True)\n",
    "                    obs, reward, done_ancien, info = test_vec_env.step(action)\n",
    "                    \n",
    "                    if done_ancien[0]:\n",
    "                        final_info = info[0]\n",
    "                        done = True\n",
    "                \n",
    "                # R√©cup√©rer le portfolio final (EXACTEMENT VOTRE CODE)\n",
    "                if final_info and 'episode' in final_info:\n",
    "                    base_env_unwrapped = test_vec_env.venv.envs[0].unwrapped\n",
    "                    \n",
    "                    try:\n",
    "                        final_metrics = base_env_unwrapped.get_metrics()\n",
    "                        final_portfolio_value = final_metrics.get('Portfolio Valuation', None)\n",
    "                        \n",
    "                        if final_portfolio_value is not None:\n",
    "                            episode_portfolios.append(final_portfolio_value)\n",
    "                            episode_returns.append((final_portfolio_value - 1000) / 1000 * 100)\n",
    "                            \n",
    "                            if ep == 0:  # Afficher le premier pour debug\n",
    "                                print(f\"      √âpisode 1: ${final_portfolio_value:.2f}\")\n",
    "                    except AttributeError:\n",
    "                        print(f\"       √âpisode {ep+1}: Impossible d'acc√©der aux m√©triques\")\n",
    "                else:\n",
    "                    print(f\"       √âpisode {ep+1}: Non termin√© correctement\")\n",
    "            \n",
    "            except Exception as ep_error:\n",
    "                print(f\"      ‚ùå √âpisode {ep+1}: Erreur - {ep_error}\")\n",
    "        \n",
    "        # Fermer proprement\n",
    "        base_env_unwrapped = test_vec_env.venv.envs[0].unwrapped\n",
    "        base_env_unwrapped.close()\n",
    "        test_vec_env.close()\n",
    "        \n",
    "        # Calculer les statistiques\n",
    "        if len(episode_portfolios) >= 3:  # Au moins 3 √©pisodes r√©ussis\n",
    "            result = {\n",
    "                'name': model_name,\n",
    "                'algo': algo,\n",
    "                'num_episodes_tested': len(episode_portfolios),\n",
    "                'mean_portfolio': np.mean(episode_portfolios),\n",
    "                'std_portfolio': np.std(episode_portfolios),\n",
    "                'min_portfolio': np.min(episode_portfolios),\n",
    "                'max_portfolio': np.max(episode_portfolios),\n",
    "                'median_portfolio': np.median(episode_portfolios),\n",
    "                'mean_return_pct': np.mean(episode_returns),\n",
    "                'std_return_pct': np.std(episode_returns),\n",
    "                'success_rate': sum(1 for p in episode_portfolios if p > 1000) / len(episode_portfolios) * 100,\n",
    "            }\n",
    "            \n",
    "            portfolio_results.append(result)\n",
    "            \n",
    "            # Afficher r√©sultat\n",
    "            print(f\"\\n    R√âSULTATS ({len(episode_portfolios)}/10 √©pisodes):\")\n",
    "            print(f\"      Portfolio Moyen    : ${result['mean_portfolio']:.2f} ¬± ${result['std_portfolio']:.2f}\")\n",
    "            print(f\"      Rendement Moyen    : {result['mean_return_pct']:+.2f}% ¬± {result['std_return_pct']:.2f}%\")\n",
    "            print(f\"      Min - Max          : ${result['min_portfolio']:.2f} - ${result['max_portfolio']:.2f}\")\n",
    "            print(f\"      M√©diane            : ${result['median_portfolio']:.2f}\")\n",
    "            print(f\"      Taux de R√©ussite   : {result['success_rate']:.0f}%\")\n",
    "            print(f\"   ‚úÖ Test r√©ussi\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Trop peu d'√©pisodes compl√©t√©s ({len(episode_portfolios)}/10)\")\n",
    "            failed_models.append({'name': model_name, 'reason': f'Only {len(episode_portfolios)} episodes completed'})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erreur : {e}\")\n",
    "        failed_models.append({'name': model_name, 'reason': str(e)})\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"‚úÖ Tests termin√©s : {len(portfolio_results)}/{len(model_dirs)} mod√®les test√©s avec succ√®s\")\n",
    "\n",
    "if len(failed_models) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è {len(failed_models)} mod√®les ont √©chou√©:\")\n",
    "    for failed in failed_models:\n",
    "        print(f\"   - {failed['name']}: {failed['reason']}\")\n",
    "\n",
    "print(f\"{'='*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
