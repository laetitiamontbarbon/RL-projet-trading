{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym_trading_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym_trading_env\u001b[39;00m  \u001b[38;5;66;03m# IMPORTANT: Pour enregistrer l'environnement\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstable_baselines3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PPO, SAC\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstable_baselines3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvec_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv, VecNormalize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gym_trading_env'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_trading_env  # IMPORTANT: Pour enregistrer l'environnement\n",
    "\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "import wandb\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "print(\"âœ… Imports rÃ©ussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration WandB et des Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 1 configurations Ã  tester\n",
      "   WandB: ActivÃ©\n"
     ]
    }
   ],
   "source": [
    "# WandB (mettre False pour dÃ©sactiver)\n",
    "USE_WANDB = True  \n",
    "\n",
    "# Liste des configurations Ã  tester\n",
    "TEST_CONFIGS = [\n",
    "    # # Test 1 : Simple Return (baseline)\n",
    "    # {\n",
    "    #     \"name\": \"simple_return_baseline\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"simple_return\",\n",
    "    #     \"risk_penalty\": 0.0,\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,  # â† RÃ©duire Ã  5000 pour test ultra-rapide\n",
    "    # },\n",
    "    \n",
    "    # # Test 2 : Sharpe Conservateur\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_conservative\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.3,\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 1e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 3 : Sharpe Ã‰quilibrÃ©\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_balanced\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.1,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 4 : Sharpe Agressif\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_aggressive\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.05,\n",
    "    #     \"reward_scaling\": 2.0,\n",
    "    #     \"learning_rate\": 5e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 5 : Momentum-Based\n",
    "    # {\n",
    "    #     \"name\": \"momentum_aggressive\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"momentum_based\",\n",
    "    #     \"risk_penalty\": 0.05,\n",
    "    #     \"reward_scaling\": 2.0,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 6 : Profit avec PÃ©nalitÃ© Drawdown\n",
    "    # {\n",
    "    #     \"name\": \"profit_with_drawdown\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"profit_drawdown\",\n",
    "    #     \"risk_penalty\": 0.5,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # Test 7 : SAC avec Simple Return\n",
    "    # {\n",
    "    #     \"name\": \"sac_simple\",\n",
    "    #     \"algo\": \"SAC\",\n",
    "    #     \"reward_type\": \"simple_return\",\n",
    "    #     \"risk_penalty\": 0.0,\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 8 : SAC avec Sharpe\n",
    "    # {\n",
    "    #     \"name\": \"sac_sharpe\",\n",
    "    #     \"algo\": \"SAC\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.1,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "     {\n",
    "        \"name\": \"sharpe_balanced_v2\",\n",
    "        \"algo\": \"PPO\",\n",
    "        \"reward_type\": \"clipped_sharpe\",\n",
    "        \"risk_penalty\": 0.1,\n",
    "        \"reward_scaling\": 1.5,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 10,\n",
    "        \"ent_coef\": 0.01,  # Exploration\n",
    "        \"timesteps\": 500000,  # 50k pour meilleurs rÃ©sultats\n",
    "    },\n",
    "    \n",
    "    # # Test 2 : Sharpe Plus Agressif (moins de pÃ©nalitÃ© risque)\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_aggressive_v2\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.05,  # â† Plus faible = plus agressif\n",
    "    #     \"reward_scaling\": 2.0,  # â† Plus de scaling\n",
    "    #     \"learning_rate\": 5e-4,  # â† Learning rate plus Ã©levÃ©\n",
    "    #     \"n_steps\": 2048,\n",
    "    #     \"batch_size\": 64,\n",
    "    #     \"n_epochs\": 10,\n",
    "    #     \"ent_coef\": 0.02,  # Plus d'exploration\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "    # # Test 3 : Sharpe Conservateur (plus de pÃ©nalitÃ© risque)\n",
    "    # {\n",
    "    #     \"name\": \"sharpe_conservative_v2\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"clipped_sharpe\",\n",
    "    #     \"risk_penalty\": 0.2,  # â† Plus Ã©levÃ© = plus conservateur\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 1e-4,  # â† Learning rate plus faible\n",
    "    #     \"n_steps\": 4096,  # â† Plus de steps = plus stable\n",
    "    #     \"batch_size\": 128,  # â† Batch plus grand\n",
    "    #     \"n_epochs\": 15,  # â† Plus d'Ã©poques\n",
    "    #     \"ent_coef\": 0.005,  # Moins d'exploration\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "   \n",
    "    \n",
    "    # # Sortino Ratio (pÃ©nalise seulement downside)\n",
    "    # {\n",
    "    #     \"name\": \"sortino_balanced\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"sortino_ratio\",\n",
    "    #     \"risk_penalty\": 0.1,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"n_steps\": 2048,\n",
    "    #     \"batch_size\": 64,\n",
    "    #     \"n_epochs\": 10,\n",
    "    #     \"ent_coef\": 0.01,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "   \n",
    "    \n",
    "    # # Simple Return avec HyperparamÃ¨tres OptimisÃ©s\n",
    "    # {\n",
    "    #     \"name\": \"simple_return_optimized\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"simple_return\",\n",
    "    #     \"risk_penalty\": 0.0,  # Pas utilisÃ©\n",
    "    #     \"reward_scaling\": 1.0,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"n_steps\": 2048,\n",
    "    #     \"batch_size\": 64,\n",
    "    #     \"n_epochs\": 10,\n",
    "    #     \"ent_coef\": 0.015,  # Un peu plus d'exploration\n",
    "    #     \"timesteps\": 500000,\n",
    "    # },\n",
    "    \n",
    "   \n",
    "    # # Sharpe + Momentum Hybride\n",
    "    # {\n",
    "    #     \"name\": \"hybrid_sharpe_momentum\",\n",
    "    #     \"algo\": \"PPO\",\n",
    "    #     \"reward_type\": \"hybrid_sharpe_momentum\", \n",
    "    #     \"risk_penalty\": 0.1,\n",
    "    #     \"reward_scaling\": 1.5,\n",
    "    #     \"learning_rate\": 3e-4,\n",
    "    #     \"n_steps\": 2048,\n",
    "    #     \"batch_size\": 64,\n",
    "    #     \"n_epochs\": 10,\n",
    "    #     \"ent_coef\": 0.01,\n",
    "    #     \"timesteps\": 500000,\n",
    "    # }\n",
    "]\n",
    "\n",
    "print(f\"âœ… {len(TEST_CONFIGS)} configurations Ã  tester\")\n",
    "print(f\"   WandB: {'ActivÃ©' if USE_WANDB else 'DÃ©sactivÃ©'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de PrÃ©traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TrouvÃ© 9 datasets\n"
     ]
    }
   ],
   "source": [
    "def load_pkl(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_pickle(path)\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        for col in [\"datetime\", \"date\", \"time\", \"timestamp\", \"Timestamp\"]:\n",
    "            if col in df.columns:\n",
    "                df = df.set_index(pd.to_datetime(df[col]))\n",
    "                break\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_log_return(df: pd.DataFrame, col: str = \"close\") -> pd.Series:\n",
    "    return np.log(df[col]).diff()\n",
    "\n",
    "\n",
    "def feature_moving_average(df: pd.DataFrame, col: str = \"close\", window: int = 20) -> pd.Series:\n",
    "    return df[col].rolling(window).mean()\n",
    "\n",
    "\n",
    "def feature_volatility(df: pd.DataFrame, col: str = \"close\", window: int = 20) -> pd.Series:\n",
    "    return df[col].pct_change().rolling(window).std()\n",
    "\n",
    "\n",
    "def feature_RSI(df: pd.DataFrame, col: str = \"close\", window: int = 14) -> pd.Series:\n",
    "    delta = df[col].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    ma_up = up.rolling(window=window).mean()\n",
    "    ma_down = down.rolling(window=window).mean()\n",
    "    rs = ma_up / (ma_down + 1e-9)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def feature_MACD(df: pd.DataFrame, col: str = \"close\", fast: int = 12, slow: int = 26, signal: int = 9):\n",
    "    ema_fast = df[col].ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = df[col].ewm(span=slow, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    macd_signal = macd.ewm(span=signal, adjust=False).mean()\n",
    "    return macd, macd_signal\n",
    "\n",
    "\n",
    "def feature_OBV(df: pd.DataFrame, col_close: str = \"close\", col_volume: str = \"volume\") -> pd.Series:\n",
    "    if col_volume not in df.columns:\n",
    "        return pd.Series(0, index=df.index)\n",
    "    obv = [0]\n",
    "    for i in range(1, len(df)):\n",
    "        if df[col_close].iat[i] > df[col_close].iat[i - 1]:\n",
    "            obv.append(obv[-1] + df[col_volume].iat[i])\n",
    "        elif df[col_close].iat[i] < df[col_close].iat[i - 1]:\n",
    "            obv.append(obv[-1] - df[col_volume].iat[i])\n",
    "        else:\n",
    "            obv.append(obv[-1])\n",
    "    return pd.Series(obv, index=df.index)\n",
    "\n",
    "\n",
    "def zscore(series: pd.Series) -> pd.Series:\n",
    "    return (series - series.mean()) / (series.std() + 1e-9)\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.sort_index().drop_duplicates().dropna(how=\"all\")\n",
    "    \n",
    "    if \"close\" not in df.columns:\n",
    "        for c in [\"Close\", \"Adj Close\", \"adjclose\", \"adj_close\"]:\n",
    "            if c in df.columns:\n",
    "                df[\"close\"] = df[c]\n",
    "                break\n",
    "    if \"volume\" not in df.columns:\n",
    "        df[\"volume\"] = 0\n",
    "\n",
    "    # Features\n",
    "    df[\"feature_log_return\"] = feature_log_return(df, \"close\")\n",
    "    df[\"feature_RSI\"] = feature_RSI(df, \"close\")\n",
    "    macd, macd_signal = feature_MACD(df, \"close\")\n",
    "    df[\"feature_MACD\"] = macd\n",
    "    df[\"feature_MACD_signal\"] = macd_signal\n",
    "    \n",
    "    for w in [5, 20, 50]:\n",
    "        df[f\"feature_ma_{w}\"] = feature_moving_average(df, \"close\", w)\n",
    "    for w in [10, 20, 50]:\n",
    "        df[f\"feature_vol_{w}\"] = feature_volatility(df, \"close\", w)\n",
    "    df[\"feature_OBV\"] = feature_OBV(df, \"close\", \"volume\")\n",
    "\n",
    "    # Normalisation\n",
    "    feat_cols = [c for c in df.columns if c.startswith(\"feature_\")]\n",
    "    for c in feat_cols:\n",
    "        df[c] = df[c].ffill().fillna(0) \n",
    "        df[c] = zscore(df[c])\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "# VÃ©rifier les donnÃ©es\n",
    "dataset_dir = \"data/*.pkl\"\n",
    "files = sorted(glob.glob(dataset_dir))\n",
    "print(f\"âœ… TrouvÃ© {len(files)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de RÃ©compense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 7 fonctions de rÃ©compense disponibles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def reward_simple_return(history, **kwargs) -> float:\n",
    "    \"\"\"REWARD 1 : Simple rendement en pourcentage\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    pct_change = (current_value - previous_value) / previous_value\n",
    "    return np.clip(pct_change * 100, -10, 10)\n",
    "\n",
    "\n",
    "def reward_clipped_sharpe(history, risk_penalty=0.1, reward_scaling=1.0) -> float:\n",
    "    \"\"\"REWARD 2 : Ratio de Sharpe clippÃ©\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0 or current_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    instant_log_return = np.log(current_value / previous_value)\n",
    "    instant_log_return = np.clip(instant_log_return, -0.1, 0.1)\n",
    "    \n",
    "    WINDOW = 20\n",
    "    all_values_np = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    safe_values = np.where(all_values_np <= 0, 1e-9, all_values_np)\n",
    "    \n",
    "    if len(safe_values) > 1:\n",
    "        log_returns = np.diff(np.log(safe_values))\n",
    "    else:\n",
    "        log_returns = np.array([0.0])\n",
    "    \n",
    "    if len(log_returns) >= WINDOW:\n",
    "        volatility = np.std(log_returns[-WINDOW:])\n",
    "    else:\n",
    "        volatility = np.std(log_returns) if len(log_returns) > 1 else 1e-9\n",
    "    \n",
    "    volatility = max(volatility, 0.001)\n",
    "    sharpe = instant_log_return / (volatility * risk_penalty)\n",
    "    sharpe = np.clip(sharpe, -10, 10)\n",
    "    \n",
    "    return sharpe * reward_scaling\n",
    "\n",
    "\n",
    "def reward_momentum_based(history, risk_penalty=0.05, reward_scaling=2.0) -> float:\n",
    "    \"\"\"REWARD 3 : BasÃ© sur le momentum\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    values = np.asarray(history['portfolio_valuation'][-10:], dtype=np.float64)\n",
    "    safe_values = np.where(values <= 0, 1e-9, values)\n",
    "    \n",
    "    if len(safe_values) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    returns = np.diff(np.log(safe_values))\n",
    "    momentum = np.mean(returns[-5:]) if len(returns) >= 5 else np.mean(returns)\n",
    "    momentum_bonus = np.tanh(momentum * 10) * reward_scaling\n",
    "    \n",
    "    instant_return = np.log(safe_values[-1] / safe_values[-2])\n",
    "    instant_return = np.clip(instant_return, -0.1, 0.1)\n",
    "    \n",
    "    total_reward = (instant_return * 100) + momentum_bonus\n",
    "    return np.clip(total_reward, -10, 10)\n",
    "\n",
    "\n",
    "def reward_profit_drawdown(history, risk_penalty=0.5, reward_scaling=1.5) -> float:\n",
    "    \"\"\"REWARD 4 : Profit avec pÃ©nalitÃ© pour drawdown\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    pct_change = (current_value - previous_value) / previous_value\n",
    "    \n",
    "    all_values = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    all_values = np.where(all_values <= 0, 1e-9, all_values)\n",
    "    \n",
    "    peak_value = np.max(all_values)\n",
    "    drawdown = (current_value - peak_value) / peak_value\n",
    "    \n",
    "    reward = (pct_change * 100) - (abs(drawdown) * 100 * risk_penalty)\n",
    "    return np.clip(reward, -10, 10) * reward_scaling\n",
    "\n",
    "\n",
    "def reward_sortino_ratio(history, risk_penalty=0.1, reward_scaling=1.0) -> float:\n",
    "    \"\"\"REWARD 5 : Ratio de Sortino (pÃ©nalise seulement downside)\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0 or current_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    instant_log_return = np.log(current_value / previous_value)\n",
    "    instant_log_return = np.clip(instant_log_return, -0.1, 0.1)\n",
    "    \n",
    "    WINDOW = 20\n",
    "    all_values_np = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    safe_values = np.where(all_values_np <= 0, 1e-9, all_values_np)\n",
    "    \n",
    "    if len(safe_values) > 1:\n",
    "        log_returns = np.diff(np.log(safe_values))\n",
    "    else:\n",
    "        log_returns = np.array([0.0])\n",
    "    \n",
    "    if len(log_returns) >= WINDOW:\n",
    "        negative_returns = log_returns[-WINDOW:][log_returns[-WINDOW:] < 0]\n",
    "        downside_vol = np.std(negative_returns) if len(negative_returns) > 0 else 0.001\n",
    "    else:\n",
    "        negative_returns = log_returns[log_returns < 0]\n",
    "        downside_vol = np.std(negative_returns) if len(negative_returns) > 0 else 0.001\n",
    "    \n",
    "    downside_vol = max(downside_vol, 0.001)\n",
    "    sortino = instant_log_return / (downside_vol * risk_penalty)\n",
    "    sortino = np.clip(sortino, -10, 10)\n",
    "    \n",
    "    return sortino * reward_scaling\n",
    "\n",
    "\n",
    "def reward_calmar_ratio(history, risk_penalty=0.2, reward_scaling=1.0) -> float:\n",
    "    \"\"\"REWARD 6 : Ratio de Calmar (rendement / max drawdown)\"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    initial_value = 1000\n",
    "    total_return = (current_value - initial_value) / initial_value\n",
    "    \n",
    "    all_values = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    all_values = np.where(all_values <= 0, 1e-9, all_values)\n",
    "    \n",
    "    running_max = np.maximum.accumulate(all_values)\n",
    "    drawdowns = (all_values - running_max) / running_max\n",
    "    max_drawdown = abs(np.min(drawdowns)) if len(drawdowns) > 0 else 0.001\n",
    "    max_drawdown = max(max_drawdown, 0.01)\n",
    "    \n",
    "    calmar = total_return / (max_drawdown * risk_penalty)\n",
    "    calmar = np.clip(calmar, -10, 10)\n",
    "    \n",
    "    return calmar * reward_scaling\n",
    "\n",
    "\n",
    "def reward_hybrid_sharpe_momentum(history, risk_penalty=0.1, reward_scaling=1.5) -> float:\n",
    "    \"\"\"\n",
    "    REWARD HYBRIDE : Combine Sharpe + Momentum\n",
    "    Le meilleur des deux mondes !\n",
    "    \"\"\"\n",
    "    current_value = history['portfolio_valuation', -1]\n",
    "    \n",
    "    if len(history) < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    previous_value = history['portfolio_valuation', -2]\n",
    "    if previous_value <= 0 or current_value <= 0:\n",
    "        return -1.0\n",
    "    \n",
    "    # ========== PARTIE 1 : SHARPE ==========\n",
    "    instant_log_return = np.log(current_value / previous_value)\n",
    "    instant_log_return = np.clip(instant_log_return, -0.1, 0.1)\n",
    "    \n",
    "    WINDOW = 20\n",
    "    all_values_np = np.asarray(history['portfolio_valuation'], dtype=np.float64)\n",
    "    safe_values = np.where(all_values_np <= 0, 1e-9, all_values_np)\n",
    "    \n",
    "    if len(safe_values) > 1:\n",
    "        log_returns = np.diff(np.log(safe_values))\n",
    "    else:\n",
    "        log_returns = np.array([0.0])\n",
    "    \n",
    "    if len(log_returns) >= WINDOW:\n",
    "        volatility = np.std(log_returns[-WINDOW:])\n",
    "    else:\n",
    "        volatility = np.std(log_returns) if len(log_returns) > 1 else 1e-9\n",
    "    \n",
    "    volatility = max(volatility, 0.001)\n",
    "    sharpe_component = instant_log_return / (volatility * risk_penalty)\n",
    "    sharpe_component = np.clip(sharpe_component, -5, 5)  # Limiter Ã  Â±5\n",
    "    \n",
    "    # ========== PARTIE 2 : MOMENTUM ==========\n",
    "    values = safe_values[-10:]\n",
    "    \n",
    "    if len(values) >= 5:\n",
    "        returns = np.diff(np.log(values))\n",
    "        momentum = np.mean(returns[-5:])\n",
    "        momentum_bonus = np.tanh(momentum * 10) * 0.5  # Bonus Â±0.5 max\n",
    "    else:\n",
    "        momentum_bonus = 0\n",
    "    \n",
    "    # ========== COMBINAISON ==========\n",
    "    # 70% Sharpe + 30% Momentum\n",
    "    hybrid_reward = (0.7 * sharpe_component) + (0.3 * momentum_bonus * 10)\n",
    "    \n",
    "    return np.clip(hybrid_reward, -10, 10) * reward_scaling\n",
    "\n",
    "\n",
    "# Dictionnaire de toutes les fonctions\n",
    "REWARD_FUNCTIONS = {\n",
    "    \"simple_return\": reward_simple_return,\n",
    "    \"clipped_sharpe\": reward_clipped_sharpe,\n",
    "    \"momentum_based\": reward_momentum_based,\n",
    "    \"profit_drawdown\": reward_profit_drawdown,\n",
    "    \"sortino_ratio\": reward_sortino_ratio,\n",
    "    \"calmar_ratio\": reward_calmar_ratio,\n",
    "    \"hybrid_sharpe_momentum\" :reward_hybrid_sharpe_momentum\n",
    "}\n",
    "\n",
    "print(f\"âœ… {len(REWARD_FUNCTIONS)} fonctions de rÃ©compense disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fonction d'EntraÃ®nement avec MÃ©triques ComplÃ¨tes WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonction d'entraÃ®nement avec mÃ©triques complÃ¨tes prÃªte\n"
     ]
    }
   ],
   "source": [
    "def train_single_config(config, use_wandb=True):\n",
    "    \"\"\"EntraÃ®ne un seul modÃ¨le avec une configuration donnÃ©e\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸš€ ENTRAÃNEMENT: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Extraire les paramÃ¨tres\n",
    "    algo = config['algo']\n",
    "    reward_type = config['reward_type']\n",
    "    risk_penalty = config['risk_penalty']\n",
    "    reward_scaling = config['reward_scaling']\n",
    "    learning_rate = config['learning_rate']\n",
    "    timesteps = config['timesteps']\n",
    "    \n",
    "    # CrÃ©er la fonction de rÃ©compense\n",
    "    reward_fn = REWARD_FUNCTIONS[reward_type]\n",
    "    \n",
    "    def reward_wrapper(history):\n",
    "        return reward_fn(history, risk_penalty=risk_penalty, reward_scaling=reward_scaling)\n",
    "    \n",
    "    # CrÃ©er l'environnement\n",
    "    env = gym.make(\n",
    "        \"MultiDatasetTradingEnv\",\n",
    "        dataset_dir=\"data/*.pkl\",\n",
    "        preprocess=preprocess,\n",
    "        portfolio_initial_value=1_000,\n",
    "        trading_fees=0.1/100,\n",
    "        borrow_interest_rate=0.02/100/24,\n",
    "        reward_function=reward_wrapper,\n",
    "    )\n",
    "    \n",
    "    env.add_metric('Portfolio Valuation', lambda h: round(h['portfolio_valuation', -1], 2))\n",
    "    \n",
    "    # Wrapping\n",
    "    log_dir = f\"models/{config['name']}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    env = Monitor(env, filename=os.path.join(log_dir, \"monitor.csv\"))\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    \n",
    "    # WandB avec TensorBoard\n",
    "    tensorboard_log_dir = f\"runs/{config['name']}\"\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=\"RL-project-trading\",\n",
    "            name=config['name'],\n",
    "            config=config,\n",
    "            reinit=True,\n",
    "            sync_tensorboard=True,  # Synchronise TensorBoard\n",
    "        )\n",
    "        print(f\"  ğŸ“Š WandB: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/runs/{wandb.run.id}\")\n",
    "    \n",
    "    # Callback DÃ‰TAILLÃ‰ avec TOUTES les mÃ©triques\n",
    "    class DetailedWandbCallback(BaseCallback):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.episode_rewards = []\n",
    "            self.episode_lengths = []\n",
    "            self.portfolio_values = []\n",
    "            self.episode_returns = []\n",
    "            self.max_drawdowns = []\n",
    "            \n",
    "        def _on_step(self):\n",
    "            # Logger tous les 100 steps\n",
    "            if use_wandb and self.n_calls % 100 == 0:\n",
    "                # MÃ©triques d'entraÃ®nement de l'algorithme\n",
    "                if hasattr(self.model, 'logger') and self.model.logger:\n",
    "                    # Ces mÃ©triques viennent de l'algorithme lui-mÃªme\n",
    "                    wandb.log({\n",
    "                        \"train/learning_rate\": self.model.learning_rate,\n",
    "                        \"timesteps\": self.num_timesteps,\n",
    "                    })\n",
    "            \n",
    "            # Logger les infos d'Ã©pisode (quand un Ã©pisode se termine)\n",
    "            for idx, info in enumerate(self.locals.get('infos', [])):\n",
    "                if 'episode' in info:\n",
    "                    episode_reward = info['episode']['r']\n",
    "                    episode_length = info['episode']['l']\n",
    "                    \n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.episode_lengths.append(episode_length)\n",
    "                    \n",
    "                    try:\n",
    "                        # RÃ©cupÃ©rer les infos dÃ©taillÃ©es de l'environnement\n",
    "                        base_env = self.training_env.envs[idx].unwrapped\n",
    "                        \n",
    "                        if hasattr(base_env, 'historical_info') and len(base_env.historical_info) > 0:\n",
    "                            # Portfolio value\n",
    "                            portfolio_value = base_env.historical_info[-1].get('portfolio_valuation', 1000)\n",
    "                            self.portfolio_values.append(portfolio_value)\n",
    "                            \n",
    "                            # Calcul du rendement\n",
    "                            total_return_pct = (portfolio_value - 1000) / 1000 * 100\n",
    "                            self.episode_returns.append(total_return_pct)\n",
    "                            \n",
    "                            # Calcul du max drawdown\n",
    "                            all_portfolio_values = [h['portfolio_valuation'] for h in base_env.historical_info]\n",
    "                            running_max = np.maximum.accumulate(all_portfolio_values)\n",
    "                            drawdowns = (np.array(all_portfolio_values) - running_max) / running_max\n",
    "                            max_drawdown = np.min(drawdowns) if len(drawdowns) > 0 else 0\n",
    "                            self.max_drawdowns.append(abs(max_drawdown) * 100)\n",
    "                            \n",
    "                            # Calcul de la volatilitÃ©\n",
    "                            if len(all_portfolio_values) > 1:\n",
    "                                returns = np.diff(np.log(all_portfolio_values))\n",
    "                                volatility = np.std(returns) * 100\n",
    "                            else:\n",
    "                                volatility = 0\n",
    "                            \n",
    "                            # Ratio de Sharpe rÃ©alisÃ© (approximatif)\n",
    "                            if volatility > 0:\n",
    "                                sharpe_ratio = total_return_pct / volatility\n",
    "                            else:\n",
    "                                sharpe_ratio = 0\n",
    "                            \n",
    "                            if use_wandb:\n",
    "                                # LOG COMPLET dans WandB\n",
    "                                wandb.log({\n",
    "                                    # Ã‰pisode de base\n",
    "                                    \"episode/reward\": episode_reward,\n",
    "                                    \"episode/length\": episode_length,\n",
    "                                    \"episode/num_episodes\": len(self.episode_rewards),\n",
    "                                    \n",
    "                                    # Portfolio\n",
    "                                    \"episode/portfolio_value\": portfolio_value,\n",
    "                                    \"episode/total_return_pct\": total_return_pct,\n",
    "                                    \n",
    "                                    # Risque\n",
    "                                    \"episode/max_drawdown_pct\": abs(max_drawdown) * 100,\n",
    "                                    \"episode/volatility_pct\": volatility,\n",
    "                                    \"episode/sharpe_ratio\": sharpe_ratio,\n",
    "                                    \n",
    "                                    # Moyennes mobiles (importantes !)\n",
    "                                    \"episode/mean_reward_100\": np.mean(self.episode_rewards[-100:]),\n",
    "                                    \"episode/mean_portfolio_100\": np.mean(self.portfolio_values[-100:]),\n",
    "                                    \"episode/mean_return_100\": np.mean(self.episode_returns[-100:]),\n",
    "                                    \n",
    "                                    # MÃ©triques cumulatives\n",
    "                                    \"cumulative/total_episodes\": len(self.episode_rewards),\n",
    "                                    \"cumulative/best_portfolio\": max(self.portfolio_values),\n",
    "                                    \"cumulative/worst_portfolio\": min(self.portfolio_values),\n",
    "                                    \"cumulative/avg_episode_length\": np.mean(self.episode_lengths),\n",
    "                                    \n",
    "                                    # Timesteps\n",
    "                                    \"timesteps\": self.num_timesteps,\n",
    "                                })\n",
    "                            \n",
    "                            # Print console\n",
    "                            print(f\"  Episode {len(self.episode_rewards)}: \"\n",
    "                                  f\"Reward={episode_reward:.2f}, \"\n",
    "                                  f\"Portfolio=${portfolio_value:.2f} ({total_return_pct:+.1f}%), \"\n",
    "                                  f\"Drawdown={abs(max_drawdown)*100:.1f}%\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        # Fallback : logger au moins les rewards\n",
    "                        if use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"episode/reward\": episode_reward,\n",
    "                                \"episode/length\": episode_length,\n",
    "                                \"episode/num_episodes\": len(self.episode_rewards),\n",
    "                                \"episode/mean_reward_100\": np.mean(self.episode_rewards[-100:]),\n",
    "                                \"timesteps\": self.num_timesteps,\n",
    "                            })\n",
    "                        print(f\"  Episode {len(self.episode_rewards)}: Reward={episode_reward:.2f}\")\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    callback = DetailedWandbCallback()\n",
    "    \n",
    "    # CrÃ©er le modÃ¨le AVEC tensorboard_log\n",
    "    if algo == \"PPO\":\n",
    "        model = PPO(\n",
    "            \"MlpPolicy\", vec_env,\n",
    "            learning_rate=config.get('learning_rate', 3e-4),\n",
    "            n_steps=config.get('n_steps', 2048),\n",
    "            batch_size=config.get('batch_size', 64),\n",
    "            n_epochs=config.get('n_epochs', 10),\n",
    "            gamma=config.get('gamma', 0.99),\n",
    "            gae_lambda=config.get('gae_lambda', 0.95),\n",
    "            clip_range=config.get('clip_range', 0.2),\n",
    "            ent_coef=config.get('ent_coef', 0.01),\n",
    "            vf_coef=config.get('vf_coef', 0.5),\n",
    "            verbose=0,\n",
    "            tensorboard_log=tensorboard_log_dir,\n",
    "        )\n",
    "    elif algo == \"SAC\":\n",
    "        model = SAC(\n",
    "            \"MlpPolicy\", vec_env,\n",
    "            learning_rate=learning_rate,\n",
    "            buffer_size=50000, batch_size=256,\n",
    "            gamma=0.99, tau=0.005,\n",
    "            verbose=0,\n",
    "            tensorboard_log=tensorboard_log_dir,\n",
    "        )\n",
    "   \n",
    "    # EntraÃ®ner\n",
    "    print(f\"  Timesteps: {timesteps}\")\n",
    "    print(f\"  EntraÃ®nement en cours...\")\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    \n",
    "    # RÃ©sultats\n",
    "    results = {\n",
    "        \"name\": config['name'],\n",
    "        \"algo\": algo,\n",
    "        \"reward_type\": reward_type,\n",
    "        \"num_episodes\": len(callback.episode_rewards),\n",
    "        \"mean_reward\": np.mean(callback.episode_rewards) if callback.episode_rewards else 0,\n",
    "        \"final_portfolio\": callback.portfolio_values[-1] if callback.portfolio_values else 1000,\n",
    "        \"mean_portfolio\": np.mean(callback.portfolio_values) if callback.portfolio_values else 1000,\n",
    "        \"max_portfolio\": np.max(callback.portfolio_values) if callback.portfolio_values else 1000,\n",
    "        \"mean_return\": np.mean(callback.episode_returns) if callback.episode_returns else 0,\n",
    "        \"max_drawdown\": np.max(callback.max_drawdowns) if callback.max_drawdowns else 0,\n",
    "    }\n",
    "    \n",
    "    # Logging final dans WandB\n",
    "    if use_wandb and len(callback.episode_rewards) > 0:\n",
    "        wandb.log({\n",
    "            \"final/total_episodes\": len(callback.episode_rewards),\n",
    "            \"final/mean_reward\": results[\"mean_reward\"],\n",
    "            \"final/portfolio_value\": results[\"final_portfolio\"],\n",
    "            \"final/mean_portfolio\": results[\"mean_portfolio\"],\n",
    "            \"final/max_portfolio\": results[\"max_portfolio\"],\n",
    "            \"final/total_return_pct\": (results[\"final_portfolio\"] - 1000) / 10,\n",
    "            \"final/mean_return_pct\": results[\"mean_return\"],\n",
    "            \"final/max_drawdown_pct\": results[\"max_drawdown\"],\n",
    "        })\n",
    "        \n",
    "        # CrÃ©er un graphique rÃ©capitulatif custom\n",
    "        if len(callback.portfolio_values) > 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Portfolio evolution\n",
    "            ax1.plot(callback.portfolio_values, linewidth=2, color='green', alpha=0.7)\n",
    "            ax1.axhline(y=1000, color='red', linestyle='--', alpha=0.5, label='Initial')\n",
    "            ax1.fill_between(range(len(callback.portfolio_values)), \n",
    "                            1000, callback.portfolio_values, \n",
    "                            alpha=0.3, color='green' if callback.portfolio_values[-1] > 1000 else 'red')\n",
    "            ax1.set_title(f'Portfolio Evolution - {config[\"name\"]}')\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Portfolio Value ($)')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.legend()\n",
    "            \n",
    "            # Rewards evolution\n",
    "            ax2.plot(callback.episode_rewards, alpha=0.3, color='blue', label='Raw')\n",
    "            if len(callback.episode_rewards) > 10:\n",
    "                window = min(20, len(callback.episode_rewards) // 5)\n",
    "                smoothed = pd.Series(callback.episode_rewards).rolling(window=window).mean()\n",
    "                ax2.plot(smoothed, linewidth=2, color='darkblue', label=f'Smoothed ({window})')\n",
    "            ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "            ax2.set_title('Episode Rewards')\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Reward')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            wandb.log({f\"charts/{config['name']}_summary\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "        \n",
    "        wandb.finish()\n",
    "    \n",
    "    # Sauvegarder\n",
    "    model.save(os.path.join(log_dir, \"model.zip\"))\n",
    "    vec_env.save(os.path.join(log_dir, \"vec_normalize.pkl\"))\n",
    "    \n",
    "    # Nettoyer\n",
    "    vec_env.close()\n",
    "    \n",
    "    print(f\"  âœ… Portfolio Final: ${results['final_portfolio']:.2f} \"\n",
    "          f\"({(results['final_portfolio']-1000)/10:+.1f}%)\")\n",
    "    print(f\"  âœ… Max Drawdown: {results['max_drawdown']:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Fonction d'entraÃ®nement avec mÃ©triques complÃ¨tes prÃªte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancer tous les tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ ENTRAÃNEMENT: sharpe_balanced_v2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lilia\\AppData\\Local\\Temp\\ipykernel_1368\\243672580.py\", line 5, in <module>\n",
      "    result = train_single_config(config, use_wandb=USE_WANDB)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\lilia\\AppData\\Local\\Temp\\ipykernel_1368\\1906654240.py\", line 47, in train_single_config\n",
      "    wandb.init(\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: module 'wandb' has no attribute 'init'\n",
      "Exception ignored in: <_io.FileIO name='C:\\\\Users\\\\lilia\\\\OneDrive\\\\Documents\\\\Etude\\\\CPE Lyon\\\\5A\\\\RL\\\\RL-projet-trading\\\\models\\\\sharpe_balanced_v2\\\\monitor.csv' mode='wb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lilia\\AppData\\Local\\Temp\\ipykernel_1368\\243672580.py\", line 10, in <module>\n",
      "ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\lilia\\\\OneDrive\\\\Documents\\\\Etude\\\\CPE Lyon\\\\5A\\\\RL\\\\RL-projet-trading\\\\models\\\\sharpe_balanced_v2\\\\monitor.csv' mode='wt' encoding='cp1252'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Erreur avec sharpe_balanced_v2: module 'wandb' has no attribute 'init'\n",
      "\n",
      "âœ… 0 configurations testÃ©es avec succÃ¨s\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for config in TEST_CONFIGS:\n",
    "    try:\n",
    "        result = train_single_config(config, use_wandb=USE_WANDB)\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur avec {config['name']}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nâœ… {len(all_results)} configurations testÃ©es avec succÃ¨s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test et Comparaison de TOUS les ModÃ¨les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TEST DE PORTFOLIO FINAL - TOUS LES MODÃˆLES\n",
      "====================================================================================================\n",
      "\n",
      "âœ… TrouvÃ© 1 modÃ¨les Ã  tester\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[1/1] Test de : \n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ModÃ¨le introuvable\n",
      "\n",
      "====================================================================================================\n",
      "âœ… Tests terminÃ©s : 0/1 modÃ¨les testÃ©s avec succÃ¨s\n",
      "\n",
      "âš ï¸ 1 modÃ¨les ont Ã©chouÃ©:\n",
      "   - : Model file not found\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"TEST DE PORTFOLIO FINAL - TOUS LES MODÃˆLES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1. Trouver tous les modÃ¨les\n",
    "model_dirs = sorted(glob.glob(\"models/*/\"))\n",
    "print(f\"\\nâœ… TrouvÃ© {len(model_dirs)} modÃ¨les Ã  tester\\n\")\n",
    "\n",
    "# 2. Stocker les rÃ©sultats\n",
    "portfolio_results = []\n",
    "failed_models = []\n",
    "\n",
    "# 3. Tester chaque modÃ¨le\n",
    "for idx, model_dir in enumerate(model_dirs, 1):\n",
    "    model_name = os.path.basename(model_dir.rstrip('/'))\n",
    "    \n",
    "    print(f\"\\n{'â”€'*100}\")\n",
    "    print(f\"[{idx}/{len(model_dirs)}] Test de : {model_name}\")\n",
    "    print(f\"{'â”€'*100}\")\n",
    "    \n",
    "    try:\n",
    "        # VÃ©rifier que les fichiers existent\n",
    "        model_file = os.path.join(model_dir, \"model.zip\")\n",
    "        vec_normalize_file = os.path.join(model_dir, \"vec_normalize.pkl\")\n",
    "        \n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"   ModÃ¨le introuvable\")\n",
    "            failed_models.append({'name': model_name, 'reason': 'Model file not found'})\n",
    "            continue\n",
    "        \n",
    "        if not os.path.exists(vec_normalize_file):\n",
    "            print(f\"   vec_normalize.pkl introuvable\")\n",
    "            failed_models.append({'name': model_name, 'reason': 'vec_normalize.pkl not found'})\n",
    "            continue\n",
    "        \n",
    "        # DÃ©tecter l'algorithme\n",
    "        algo = None\n",
    "        if 'sac' in model_name.lower():\n",
    "            algo = 'SAC'\n",
    "        else:\n",
    "            algo = 'PPO'\n",
    "        \n",
    "        print(f\"   ğŸ“¦ Algorithme : {algo}\")\n",
    "        \n",
    "        # Charger le modÃ¨le\n",
    "        if algo == 'PPO':\n",
    "            loaded_model = PPO.load(model_file)\n",
    "        elif algo == 'SAC':\n",
    "            loaded_model = SAC.load(model_file)\n",
    "       \n",
    "        print(f\"âœ… ModÃ¨le chargÃ©\")\n",
    "        \n",
    "        # Trouver la config correspondante\n",
    "        config = None\n",
    "        for test_config in TEST_CONFIGS:\n",
    "            if test_config['name'] == model_name:\n",
    "                config = test_config\n",
    "                break\n",
    "        \n",
    "        # Si pas de config, utiliser simple_return\n",
    "        if config is None:\n",
    "            print(f\"   Config non trouvÃ©e, utilisation de simple_return\")\n",
    "            reward_fn = reward_simple_return\n",
    "            def reward_wrapper(history):\n",
    "                return reward_fn(history)\n",
    "        else:\n",
    "            reward_fn = REWARD_FUNCTIONS[config['reward_type']]\n",
    "            def reward_wrapper(history):\n",
    "                return reward_fn(history, \n",
    "                               risk_penalty=config.get('risk_penalty', 0.1), \n",
    "                               reward_scaling=config.get('reward_scaling', 1.0))\n",
    "        \n",
    "        # CrÃ©er un environnement de test (comme dans le code original)\n",
    "        test_env = gym.make(\n",
    "            \"MultiDatasetTradingEnv\",\n",
    "            dataset_dir=\"data/*.pkl\",\n",
    "            preprocess=preprocess,\n",
    "            portfolio_initial_value=1_000,\n",
    "            trading_fees=0.1/100,\n",
    "            borrow_interest_rate=0.02/100/24,\n",
    "            reward_function=reward_wrapper,\n",
    "        )\n",
    "        \n",
    "        test_env.add_metric('Portfolio Valuation', lambda h: round(h['portfolio_valuation', -1], 2))\n",
    "        \n",
    "        # Wrapping (EXACTEMENT comme le code)\n",
    "        test_env = Monitor(test_env)\n",
    "        test_vec_env = DummyVecEnv([lambda: test_env])\n",
    "        \n",
    "        # Charger la normalisation COMPLÃˆTE\n",
    "        test_vec_env = VecNormalize.load(vec_normalize_file, test_vec_env)\n",
    "        \n",
    "        print(f\"âœ… Environnement crÃ©Ã© et normalisÃ©\")\n",
    "        \n",
    "        # Tester sur 10 Ã©pisodes\n",
    "        print(f\"Test sur 10 Ã©pisodes...\")\n",
    "        \n",
    "        episode_portfolios = []\n",
    "        episode_returns = []\n",
    "        \n",
    "        for ep in range(10):\n",
    "            try:\n",
    "                # EXACTEMENT LE CODE DE LA CELL 9\n",
    "                obs = test_vec_env.reset()\n",
    "                done = False\n",
    "                final_info = None\n",
    "                \n",
    "                while not done:\n",
    "                    action, _ = loaded_model.predict(obs, deterministic=True)\n",
    "                    obs, reward, done_ancien, info = test_vec_env.step(action)\n",
    "                    \n",
    "                    if done_ancien[0]:\n",
    "                        final_info = info[0]\n",
    "                        done = True\n",
    "                \n",
    "                # RÃ©cupÃ©rer le portfolio final (EXACTEMENT LE CODE)\n",
    "                if final_info and 'episode' in final_info:\n",
    "                    base_env_unwrapped = test_vec_env.venv.envs[0].unwrapped\n",
    "                    \n",
    "                    try:\n",
    "                        final_metrics = base_env_unwrapped.get_metrics()\n",
    "                        final_portfolio_value = final_metrics.get('Portfolio Valuation', None)\n",
    "                        \n",
    "                        if final_portfolio_value is not None:\n",
    "                            episode_portfolios.append(final_portfolio_value)\n",
    "                            episode_returns.append((final_portfolio_value - 1000) / 1000 * 100)\n",
    "                            \n",
    "                            if ep == 0:  # Afficher le premier pour debug\n",
    "                                print(f\"      Ã‰pisode 1: ${final_portfolio_value:.2f}\")\n",
    "                    except AttributeError:\n",
    "                        print(f\"       Ã‰pisode {ep+1}: Impossible d'accÃ©der aux mÃ©triques\")\n",
    "                else:\n",
    "                    print(f\"       Ã‰pisode {ep+1}: Non terminÃ© correctement\")\n",
    "            \n",
    "            except Exception as ep_error:\n",
    "                print(f\"      âŒ Ã‰pisode {ep+1}: Erreur - {ep_error}\")\n",
    "        \n",
    "        # Fermer proprement\n",
    "        base_env_unwrapped = test_vec_env.venv.envs[0].unwrapped\n",
    "        base_env_unwrapped.close()\n",
    "        test_vec_env.close()\n",
    "        \n",
    "        # Calculer les statistiques\n",
    "        if len(episode_portfolios) >= 3:  # Au moins 3 Ã©pisodes rÃ©ussis\n",
    "            result = {\n",
    "                'name': model_name,\n",
    "                'algo': algo,\n",
    "                'num_episodes_tested': len(episode_portfolios),\n",
    "                'mean_portfolio': np.mean(episode_portfolios),\n",
    "                'std_portfolio': np.std(episode_portfolios),\n",
    "                'min_portfolio': np.min(episode_portfolios),\n",
    "                'max_portfolio': np.max(episode_portfolios),\n",
    "                'median_portfolio': np.median(episode_portfolios),\n",
    "                'mean_return_pct': np.mean(episode_returns),\n",
    "                'std_return_pct': np.std(episode_returns),\n",
    "                'success_rate': sum(1 for p in episode_portfolios if p > 1000) / len(episode_portfolios) * 100,\n",
    "            }\n",
    "            \n",
    "            portfolio_results.append(result)\n",
    "            \n",
    "            # Afficher rÃ©sultat\n",
    "            print(f\"\\n    RÃ‰SULTATS ({len(episode_portfolios)}/10 Ã©pisodes):\")\n",
    "            print(f\"      Portfolio Moyen    : ${result['mean_portfolio']:.2f} Â± ${result['std_portfolio']:.2f}\")\n",
    "            print(f\"      Rendement Moyen    : {result['mean_return_pct']:+.2f}% Â± {result['std_return_pct']:.2f}%\")\n",
    "            print(f\"      Min - Max          : ${result['min_portfolio']:.2f} - ${result['max_portfolio']:.2f}\")\n",
    "            print(f\"      MÃ©diane            : ${result['median_portfolio']:.2f}\")\n",
    "            print(f\"      Taux de RÃ©ussite   : {result['success_rate']:.0f}%\")\n",
    "            print(f\"   âœ… Test rÃ©ussi\")\n",
    "        else:\n",
    "            print(f\"   âŒ Trop peu d'Ã©pisodes complÃ©tÃ©s ({len(episode_portfolios)}/10)\")\n",
    "            failed_models.append({'name': model_name, 'reason': f'Only {len(episode_portfolios)} episodes completed'})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Erreur : {e}\")\n",
    "        failed_models.append({'name': model_name, 'reason': str(e)})\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"âœ… Tests terminÃ©s : {len(portfolio_results)}/{len(model_dirs)} modÃ¨les testÃ©s avec succÃ¨s\")\n",
    "\n",
    "if len(failed_models) > 0:\n",
    "    print(f\"\\nâš ï¸ {len(failed_models)} modÃ¨les ont Ã©chouÃ©:\")\n",
    "    for failed in failed_models:\n",
    "        print(f\"   - {failed['name']}: {failed['reason']}\")\n",
    "\n",
    "print(f\"{'='*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
