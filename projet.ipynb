{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL -- Projet \"Trading automatique\"\n",
    "\n",
    "Ce notebook contient du code de base et quelques explications pour vous aider sur ce sujet.\n",
    "\n",
    "Vous êtes libres de réaliser ce projet avec des scripts Python ou des Jupyter Notebooks, à votre convenance.\n",
    "\n",
    "Vous devez télécharger les paquets Python suivants :\n",
    "\n",
    "```sh\n",
    "pip install gymnasium\n",
    "pip install pandas\n",
    "pip install gym-trading-env-continuous\n",
    "```\n",
    "\n",
    "Vous utiliserez l'environnement `gym-trading-env-continuous`, qui est un *fork* de [Gym Trading Env](https://gym-trading-env.readthedocs.io/en/latest/index.html). La différence majeure est expliquée dans ce document ; la documentation originelle reste utilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation des données de simulation\n",
    "\n",
    "Les données sont dans un format binaire (Pickle) que vous pouvez lire avec Pandas. Vous devez vous assurer que les données sont triées par date.\n",
    "\n",
    "Des étapes de prétraitement peuvent aider votre apprentissage, par exemple, supprimer les doublons, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>date_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-08-18 07:00:00</th>\n",
       "      <td>430.00</td>\n",
       "      <td>435.00</td>\n",
       "      <td>410.00</td>\n",
       "      <td>430.30</td>\n",
       "      <td>487.154463</td>\n",
       "      <td>2020-08-18 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 08:00:00</th>\n",
       "      <td>430.27</td>\n",
       "      <td>431.79</td>\n",
       "      <td>430.27</td>\n",
       "      <td>430.80</td>\n",
       "      <td>454.176153</td>\n",
       "      <td>2020-08-18 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 09:00:00</th>\n",
       "      <td>430.86</td>\n",
       "      <td>431.13</td>\n",
       "      <td>428.71</td>\n",
       "      <td>429.35</td>\n",
       "      <td>1183.710884</td>\n",
       "      <td>2020-08-18 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 10:00:00</th>\n",
       "      <td>429.75</td>\n",
       "      <td>432.69</td>\n",
       "      <td>428.59</td>\n",
       "      <td>431.90</td>\n",
       "      <td>1686.183227</td>\n",
       "      <td>2020-08-18 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18 11:00:00</th>\n",
       "      <td>432.09</td>\n",
       "      <td>432.89</td>\n",
       "      <td>426.99</td>\n",
       "      <td>427.45</td>\n",
       "      <td>1980.692724</td>\n",
       "      <td>2020-08-18 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       open    high     low   close       volume  \\\n",
       "date_open                                                          \n",
       "2020-08-18 07:00:00  430.00  435.00  410.00  430.30   487.154463   \n",
       "2020-08-18 08:00:00  430.27  431.79  430.27  430.80   454.176153   \n",
       "2020-08-18 09:00:00  430.86  431.13  428.71  429.35  1183.710884   \n",
       "2020-08-18 10:00:00  429.75  432.69  428.59  431.90  1686.183227   \n",
       "2020-08-18 11:00:00  432.09  432.89  426.99  427.45  1980.692724   \n",
       "\n",
       "                             date_close  \n",
       "date_open                                \n",
       "2020-08-18 07:00:00 2020-08-18 08:00:00  \n",
       "2020-08-18 08:00:00 2020-08-18 09:00:00  \n",
       "2020-08-18 09:00:00 2020-08-18 10:00:00  \n",
       "2020-08-18 10:00:00 2020-08-18 11:00:00  \n",
       "2020-08-18 11:00:00 2020-08-18 12:00:00  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de *features*\n",
    "\n",
    "Vous pouvez également rajouter de nouvelles données au DataFrame pour créer de nouvelles *features* que l'agent pourra utiliser.\n",
    "Voir pour cela la [doc](https://gym-trading-env.readthedocs.io/en/latest/features.html).\n",
    "\n",
    "Chaque nouvelle *feature* doit commencer par `feature_` pour être détectée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df['feature_close'] = (df['close'] - df['close'].mean()) / df['close'].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess(pd.read_pickle('./data/binance-ETHUSD-1h.pkl'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, l'agent ne reçoit comme *features* que sa dernière *position* (voir le paragraphe suivant), ce qui ne sera certainement pas suffisant ! À vous d'ajouter les *features* qui seront pertinentes pour que l'agent apprenne la politique optimale..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctionnement des actions\n",
    "\n",
    "Une action est une **position**, c'est-à-dire un ratio entre la proportion d'*assets* (exemple : ETH) et la proportion de *fiat* (exemple : USD) dans le portefeuille.\n",
    "Ainsi, la position `0.5` consiste à avoir exactement 50% d'ETH et 50% d'USD (en vendant l'un ou l'autre pour arriver à ce ratio). `0.1` consiste à avoir 10% d'ETH et 90% d'USD.\n",
    "\n",
    "Il existe des positions un peu plus complexes :\n",
    "\n",
    "- `< 0` : une position inférieure à 0 va vendre encore plus d'ETH que le portefeuille n'en contient, pour obtenir des USD. Cela nécessite un emprunt, qui sera remboursé avec un intérêt.\n",
    "- `> 1` : une position supérieure à 1 va dépenser encore plus d'USD que le portefeuille n'en contient, pour acheter des ETH. Cela nécessite également un emprunt.\n",
    "\n",
    "Ces positions (qui sont appelées *short* et *margin* en finance) peuvent faire gagner beaucoup à votre agent, mais démultiplient les risques également. Si votre agent fait une bonne affaire, vous pouvez vendre à un prix élevé, racheter quand le prix est plus faible, et rembourser l'emprunt en empochant la différence. En revanche, si votre agent fait une mauvaise affaire, et doit vider son portefeuille pour rembourser l'emprunt, vous perdez automatiquement (`terminated=True`).\n",
    "\n",
    "### Actions continues\n",
    "\n",
    "Par rapport à l'environnement `gym-trading-env` d'origine, la version que je vous fournis permet de spécifier directement une position comme action, c'est-à-dire un nombre flottant. Votre agent a donc un contrôle précis sur la position désirée. Cela rajoute de la flexibilité mais rend l'apprentissage beaucoup plus difficile.\n",
    "\n",
    "Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.023873   0.88       0.8800949]\n",
      "{'idx': 1, 'step': 1, 'date': np.datetime64('2023-11-21T15:30:00.000000000'), 'position_index': None, 'position': 0.88, 'real_position': np.float64(0.8800948613860823), 'data_date_close': Timestamp('2023-11-21 16:30:00'), 'data_volume': 257387382, 'data_close': 4531.02978515625, 'data_open': 4527.009765625, 'data_low': 4525.509765625, 'data_high': 4533.509765625, 'portfolio_valuation': np.float64(1000.0163489547764), 'portfolio_distribution_asset': np.float64(0.1942404468186077), 'portfolio_distribution_fiat': np.float64(119.9070989376064), 'portfolio_distribution_borrowed_asset': 0, 'portfolio_distribution_borrowed_fiat': 0, 'portfolio_distribution_interest_asset': 0.0, 'portfolio_distribution_interest_fiat': 0.0, 'reward': np.float64(1.634882113371942e-05)}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "# On veut une position de 88% ETH / 12% USD\n",
    "obs, reward, terminated, truncated, info = env.step(0.88)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, l'espace des actions est limité à $[-1, 2]$ pour que votre agent ne puisse emprunter que jusqu'à 100%. Vous pouvez empêcher votre agent de prendre de telles positions, ou limiter le risque, en contrôlant les bornes autorisées des actions.\n",
    "\n",
    "Par exemple, en clippant l'action dans l'intervalle $[0,1]$, vous empêchez l'agent de faire des emprunts.\n",
    "\n",
    "À l'inverse, vous pouvez augmenter l'intervalle pour permettre des emprunts plus risqués, mais qui peuvent rapporter plus. À vous de choisir !\n",
    "\n",
    "Vous pouvez changer les bornes via le paramètre `position_range` du constructeur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    position_range=(0, 1),  # ICI : (borne min, borne max)\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez aussi modifier l'action en sortie de votre algorithme d'apprentissage, de la manière que vous souhaitez (clipping, interpolation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions discrètes\n",
    "\n",
    "Pour simplifier l'apprentissage, vous pouvez utiliser le *wrapper* `gym_trading_env.wrapper.DiscreteActionsWrapper` que je vous fournis, et qui permet de revenir au fonctionnement d'origine de l'environnement `gym-trading-env`. Vous devrez alors spécifier l'ensemble des positions possibles, puis votre agent choisira une position parmi cette liste à chaque pas de temps.\n",
    "Par exemple, si la liste des positions est `[0, 0.5, 1]` et que l'action choisie est `1`, cela veut dire qu'on veut la position qui correspond au 2e élément de la liste, soit `0.5` (50%/50%).\n",
    "\n",
    "Vous pouvez rajouter autant d'actions que vous voulez, par exemple `[0, 0.25, 0.5, 1]` ou encore tous les 0.1 entre 0 et 1, etc. Plus il y a d'actions possibles, plus votre agent aura de choix (flexibilité), donc plus son comportement pourra être complexe, mais cela rajoute de la difficulté durant l'entraînement.\n",
    "\n",
    "N'oubliez pas que vous pouvez autoriser les positions avec emprunt en ajoutant des nombres inférieurs à 0 ou supérieurs à 1 à la liste autorisée.\n",
    "\n",
    "Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3615264  0.25       0.2507494]\n",
      "{'idx': 1, 'step': 1, 'date': np.datetime64('2023-11-21T14:00:00.000000000'), 'position_index': 2, 'position': 0.25, 'real_position': np.float64(0.250749400488757), 'data_date_close': Timestamp('2023-11-21 15:00:00'), 'data_volume': 36358, 'data_close': 2007.5999755859375, 'data_open': 1999.699951171875, 'data_low': 1999.300048828125, 'data_high': 2009.699951171875, 'portfolio_valuation': np.float64(1000.5117065718047), 'portfolio_distribution_asset': np.float64(0.12496399365199337), 'portfolio_distribution_fiat': np.float64(749.6339959669415), 'portfolio_distribution_borrowed_asset': 0, 'portfolio_distribution_borrowed_fiat': 0, 'portfolio_distribution_interest_asset': 0.0, 'portfolio_distribution_interest_fiat': 0.0, 'reward': np.float64(0.0005115756946421896)}\n"
     ]
    }
   ],
   "source": [
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "\n",
    "# Vous pouvez aussi appeler le wrapper `env` pour faire plus simple\n",
    "# Ici, je fais explicitement la distinction entre `wrapper` et `env`\n",
    "wrapper = DiscreteActionsWrapper(env, positions=[-1, 0, 0.25, 0.5, 0.75, 1, 2])\n",
    "obs, _ = wrapper.reset()\n",
    "# On veut une position de 25% ETH / 75% USD ; cela correspond à la position\n",
    "# d'index 2 dans la liste ci-dessus\n",
    "obs, reward, terminated, truncated, info = wrapper.step(2)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que, quand les actions continues sont utilisées, la variable `position_index` du dictionnaire `info` n'est pas disponible (c'est logique)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changement de la fonction de récompense\n",
    "\n",
    "Vous pouvez changer la fonction de récompense pour améliorer l'apprentissage de l'agent.\n",
    "Dans tous les cas, vous serez évalué(e)s sur la valuation du portefeuille à la fin de l'épisode (voir [ci-dessous](#évaluation)), mais cette simple mesure n'est peut-être pas la meilleure fonction de récompense.\n",
    "D'autres fonctions peuvent encourager l'agent à mieux apprendre, en explorant diverses possibilités, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(history):\n",
    "    return history['portfolio_valuation', -1]\n",
    "\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    portfolio_initial_value=1_000,\n",
    "    trading_fees=0.1/100,\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    "    # On spécifie la fonction de récompense\n",
    "    reward_function=reward_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déroulément d'un épisode\n",
    "\n",
    "Un épisode se déroule jusqu'à ce que :\n",
    "\n",
    "- l'agent atteigne la fin des données d'entraînement (nous n'avons plus de nouvelle donnée) => `truncated=True`\n",
    "\n",
    "- la valeur du portefeuille atteint 0 (l'agent a perdu tout l'argent) => `terminated=True`\n",
    "\n",
    "Vous devrez probablement entraîner l'agent sur plusieurs épisodes avant que son comportement ne converge.\n",
    "\n",
    "Pour éviter de sur-apprendre (*overfit*), vous devrez utiliser plusieurs jeux de données via [MultiDatasetTradingEnv](https://gym-trading-env.readthedocs.io/en/latest/multi_datasets.html).\n",
    "\n",
    "Dans ce cas, chaque épisode utilisera un jeu de données différent (en bouclant si vous demandez plus d'épisodes qu'il n'y a de jeux de données). Vous pouvez accéder au nom du jeu de données de l'épisode en cours via `env.name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode n˚1 -- Jeu de donnée yfinance-CAC40-1h.pkl\n",
      "Market Return : 10.54%   |   Portfolio Return : -99.21%   |   \n",
      "Épisode terminé\n",
      "Episode n˚2 -- Jeu de donnée binance-BTCUSD-1h.pkl\n",
      "Market Return : 677.81%   |   Portfolio Return : -100.00%   |   \n",
      "Épisode terminé\n"
     ]
    }
   ],
   "source": [
    "nb_episodes = 2\n",
    "for episode in range(1, nb_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    print(f'Episode n˚{episode} -- Jeu de donnée {env.name}')\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        print('Argent perdu')\n",
    "    elif truncated:\n",
    "        print('Épisode terminé')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation\n",
    "\n",
    "Afin de disposer d'un critère simple pour comparer les différentes solutions, nous utiliserons la valeur du portefeuille (`portfolio_valuation`).\n",
    "C'est assez simple : on veut que l'agent ait gagné le plus d'argent à la fin de la simulation.\n",
    "\n",
    "Vous pouvez ajouter ce critère à la liste des métriques affichées à la fin de chaque épisode, pour que ce soit plus visible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 27.70%   |   Portfolio Return : -98.62%   |   Portfolio Valuation : 13.78   |   \n"
     ]
    }
   ],
   "source": [
    "def metric_portfolio_valuation(history):\n",
    "    return round(history['portfolio_valuation', -1], 2)\n",
    "\n",
    "env.add_metric('Portfolio Valuation', metric_portfolio_valuation)\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque l'environnement peut se dérouler sur plusieurs épisodes (1 par jeu de données), vous devrez calculer la **moyenne des `portfolio_valuation`** sur l'ensemble des jeux de données possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Pour que ce soit honnête, vous **devez initialiser l'environnement avec les contraintes** imposées dans le sujet :\n",
    "\n",
    "- une valeur initiale du portefeuille de `1000` ;\n",
    "- des frais de 0.1% par transaction ;\n",
    "- un taux d'intérêt de 0.02% par jour soit 0.02/100/24 par heure.\n",
    "\n",
    "Sinon, il est beaucoup plus simple d'augmenter la valeur finale...\n",
    "\n",
    "```py\n",
    "env = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=\"data/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    # LIGNES SUIVANTES :\n",
    "    # Valeur initiale du portefeuille\n",
    "    portfolio_initial_value=1_000,\n",
    "    # Frais de transactions\n",
    "    trading_fees=0.1/100,\n",
    "    # Intérêts sur les prêts\n",
    "    borrow_interest_rate=0.02/100/24,\n",
    ")\n",
    "```\n",
    "\n",
    "Vous pouvez également accéder à la métrique de `portfolio_valuation` à la fin d'une simulation, si vous voulez par exemple l'ajouter à votre *run* WandB :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.776557403283697\n"
     ]
    }
   ],
   "source": [
    "portfolio_valuation = env.historical_info['portfolio_valuation', -1]\n",
    "# Si on avait WandB :\n",
    "# run.summary['portfolio_valuation'] = portfolio_valuation\n",
    "# On simule ça par un simple print...\n",
    "print(portfolio_valuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou bien, pour récupérer les métriques calculées par l'environnement (cela peut être utile pour les ajouter à WandB) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Market Return': '27.70%', 'Portfolio Return': '-98.62%', 'Portfolio Valuation': np.float64(13.78)}\n",
      "13.78\n"
     ]
    }
   ],
   "source": [
    "metrics = env.get_metrics()\n",
    "print(metrics)\n",
    "portfolio_valuation = metrics['Portfolio Valuation']\n",
    "print(portfolio_valuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conseils\n",
    "\n",
    "À part les quelques contraintes présentées dans ce fichier (et rappelées sur la page du projet), vous êtes assez libres !\n",
    "\n",
    "Votre algorithme de RL peut être arbitrairement simple ou complexe. Je liste ici quelques conseils ou pistes, que vous pouvez explorer :\n",
    "\n",
    "- *Features* : Par défaut, votre agent n'utilise que le prix de l'*asset* (`close`) comme *feature* pour la prise de décision. Vous pouvez ajouter les *features* que vous voulez. En particulier, des métriques spécifiques à la finance peuvent être intéressantes, par exemple pour déterminer le risque que le prix change brutalement (à la hausse ou à la baisse)...\n",
    "\n",
    "- Algorithme : Vous pouvez utiliser des algorithmes existants, ou en inventer un nouveau. N'hésitez pas à ré-utiliser tout ce que vous avez appris en *Machine Learning* et *Deep Learning*. Typiquement, les données financières sont des données temporelles : certains réseaux de neurones sont plus appropriés que d'autres pour ce genre de tâche...\n",
    "\n",
    "- Configuration de l'environnement : L'environnement est très extensible ! Vous pouvez par exemple ajouter des *features* dynamiques (pas seulement calculées lors du prétraitement). La [documentation](https://gym-trading-env.readthedocs.io/en/latest/index.html) est très claire et très complète.\n",
    "\n",
    "Vous pouvez vous inspirer de travaux existants trouvés sur l'Internet à condition de **citer votre source**. Utiliser le travail de quelqu'un d'autre sans le citer sera considéré comme du plagiat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
